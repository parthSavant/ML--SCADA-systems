{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________\n",
      "START Data Pre-processing\n",
      "____________________________________________\n",
      "Encoding labels...\n",
      "Encoding addressess...\n",
      "Encoding functions...\n",
      "Encoding command responses...\n",
      "Extracting payload features...\n",
      "[('setpoint', '<f8'), ('gain', '<f8'), ('reset rate', '<f8'), ('deadband', '<f8'), ('cycle time', '<f8'), ('rate', '<f8'), ('system mode', '<f8'), ('control scheme', '<f8'), ('pump', '<f8'), ('solenoid', '<f8'), ('pressure measurement', '<f8')]\n",
      "\tAdding length...\n",
      "\tAdding crc rate...\n",
      "\tAdding timestamp differences...\n",
      "\n",
      "\tSplitting dataset...\n",
      "Encoding function...\n",
      "Imputing payload using indicators\n",
      "Additional features were found!\n",
      "Encoding function...\n",
      "Imputing payload using indicators\n",
      "Additional features were found!\n",
      "Encoding function...\n",
      "Imputing payload using indicators\n",
      "Additional features were found!\n",
      "Encoding function...\n",
      "Imputing payload using indicators\n",
      "Additional features were found!\n",
      "Encoding function...\n",
      "Imputing payload using indicators\n",
      "Additional features were found!\n",
      "(219701, 57) (219701, 1)\n",
      "(54927, 57) (54927, 1)\n",
      "____________________________________________\n",
      "END Data Pre-processing\n",
      "____________________________________________\n",
      "____________________________________________\n",
      "START Model Run\n",
      "____________________________________________\n",
      "[0. 1.]\n",
      "1\n",
      "Label dimension is: 1\n",
      "Input dimension - training data: 164776\n",
      "Input dimension - validation data: 54925\n",
      "Input dimension - test data: 54927\n",
      "n_estimators: 71\n",
      "criterion: gini\n",
      "max_depth: 86\n",
      "bootstrap: False\n",
      "max max_features: 0.5\n",
      "Accuracy on trainning data = 1.0\n",
      "Accuray on validation data = 0.9837960855712335\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9849    0.9945    0.9896     42734\n",
      "         1.0     0.9799    0.9464    0.9629     12191\n",
      "\n",
      "    accuracy                         0.9838     54925\n",
      "   macro avg     0.9824    0.9704    0.9763     54925\n",
      "weighted avg     0.9838    0.9838    0.9837     54925\n",
      "\n",
      "--- 13.726659774780273 seconds ---\n",
      "Input dimension - training data: 164776\n",
      "Input dimension - validation data: 54925\n",
      "Input dimension - test data: 54927\n",
      "n_estimators: 51\n",
      "criterion: gini\n",
      "max_depth: 88\n",
      "bootstrap: False\n",
      "max max_features: 0.5\n",
      "Accuracy on trainning data = 1.0\n",
      "Accuray on validation data = 0.9840509786071916\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9850    0.9946    0.9898     42734\n",
      "         1.0     0.9805    0.9470    0.9634     12191\n",
      "\n",
      "    accuracy                         0.9841     54925\n",
      "   macro avg     0.9827    0.9708    0.9766     54925\n",
      "weighted avg     0.9840    0.9841    0.9840     54925\n",
      "\n",
      "--- 23.753109455108643 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#--------------------#\n",
    "# Import statements #\n",
    "#--------------------#\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.recfunctions import repack_fields\n",
    "\n",
    "import time\n",
    "import scipy, argparse, itertools, os\n",
    "from scipy.io import arff\n",
    "\n",
    "import sklearn.utils\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Output\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Visualizations\n",
    "import matplotlib.pyplot as mp\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly as pl\n",
    "import plotly.offline as plo\n",
    "import plotly.graph_objs as plg\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "#Models\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, accuracy_score,confusion_matrix\n",
    "\n",
    "#--------------------#\n",
    "# Classes & Methods #\n",
    "#--------------------#\n",
    "\n",
    "#Class: MaskedStandardScaler\n",
    "class MSS:\n",
    "\n",
    "  def fit_transform(self, Xs):\n",
    "    self.mean = Xs.mean(axis=0)\n",
    "    self.std = Xs.std(axis=0)\n",
    "    return self.transform(Xs)\n",
    "\n",
    "  def transform(self, Xs):\n",
    "    return (Xs - self.mean) / self.std\n",
    "\n",
    "#Class: MaskedMinMaxScaler\n",
    "class MMMS:\n",
    "\n",
    "  def fit_transform(self, Xs):\n",
    "    self.min = Xs.min(axis=0)\n",
    "    self.max = Xs.max(axis=0)\n",
    "    return self.transform(Xs)\n",
    "\n",
    "  def transform(self, Xs):\n",
    "    return (Xs - self.min) / (self.max - self.min)\n",
    "\n",
    "def tocategoricalwithnans(Xs, n_cats):\n",
    "  categorized = np.zeros((len(Xs), n_cats))\n",
    "  for n, v in enumerate(np.array(Xs)):\n",
    "    if not np.isnan(v):\n",
    "      categorized[n, int(v)] = 1 \n",
    "  return categorized\n",
    "\n",
    "def pairwise(iterable):\n",
    "  a, b = itertools.tee(iterable)\n",
    "  next(b, None)\n",
    "  return zip(a, b)\n",
    "\n",
    "def splitdataset(data, labels, train_per_split, val_per_split, test_per_split):\n",
    "  assert (train_per_split + val_per_split + test_per_split) <= 1.0\n",
    "  \n",
    "  train_end_index = int(data.shape[0] * train_per_split)\n",
    "  valid_end_index = int(data.shape[0] * val_per_split) + train_end_index\n",
    "\n",
    "  X_train, Y_train = data[:train_end_index], labels[:train_end_index]\n",
    "  X_valid, Y_valid = data[train_end_index:valid_end_index], labels[train_end_index:valid_end_index]\n",
    "  X_test,  Y_test  = data[valid_end_index:], labels[valid_end_index:]\n",
    "  \n",
    "  return (X_train, X_valid, X_test), (Y_train, Y_valid, Y_test)\n",
    "\n",
    "def normalize(Xs, op, train_data_model):\n",
    "\n",
    "  ops = { \n",
    "    'mean'     : preprocessing.StandardScaler,\n",
    "    'minmax'   : preprocessing.MinMaxScaler,\n",
    "    'm-mean'   : MSS,\n",
    "    'm-minmax' : MMMS\n",
    "  }\n",
    "\n",
    "  if not train_data_model:\n",
    "    model = ops[op]()\n",
    "    Xs = model.fit_transform(Xs)\n",
    "    return Xs, model\n",
    "  else:\n",
    "    return train_data_model.transform(Xs), None\n",
    "\n",
    "def getclustermodelk(model, flags):\n",
    "  if flags.payload_kmeans_imputed:\n",
    "    return model.cluster_centers_.shape[0]\n",
    "  elif flags.payload_gmm_imputed:\n",
    "    return model.weights_.shape[0]\n",
    "\n",
    "def clusterpayloadfeatures(payloads, flags, train_data_model):\n",
    "  groups = grouppayloads(payloads)\n",
    "  groups_ids = [np.where(groups == i)[0] for i in range(3)]\n",
    "\n",
    "  only_nans = np.zeros_like(groups_ids[0])\n",
    "  only_pres = payloads[groups_ids[1]][:,-1].reshape((-1, 1))\n",
    "  wo_pres   = payloads[groups_ids[2]][:,:-1]\n",
    "\n",
    "  if not train_data_model:\n",
    "    \n",
    "    if flags.payload_kmeans_imputed:\n",
    "      print('Imputing payload with kmeans')\n",
    "      k1, k2 = flags.payload_kmeans_imputed\n",
    "      cluster_func = clusterkmeans\n",
    "    elif flags.payload_gmm_imputed:\n",
    "      print('Imputing payload with GMM')\n",
    "      k1, k2 = flags.payload_gmm_imputed\n",
    "      cluster_func = clustergmm\n",
    "\n",
    "    only_pres, only_pressure_cluster_model = cluster_func(only_pres, \\\n",
    "      'pressure', elbow=flags.elbow, k=k1)\n",
    "    wo_pres, wo_pressure_cluster_model = cluster_func(wo_pres, \\\n",
    "      'remaining payload features', elbow=flags.elbow, k=k2)\n",
    "\n",
    "  else:\n",
    "    only_pressure_cluster_model = train_data_model['only_pressure']\n",
    "    wo_pressure_cluster_model = train_data_model['wo_pressure']\n",
    "    only_pres = only_pressure_cluster_model.predict(only_pres)\n",
    "    wo_pres = wo_pressure_cluster_model.predict(wo_pres)\n",
    "\n",
    "  only_pres += 1\n",
    "  wo_pres += (getclustermodelk(only_pressure_cluster_model, flags) + 1)\n",
    "\n",
    "  combined_cat = np.concatenate((only_nans, only_pres, wo_pres))\n",
    "  combined_ind = np.concatenate(groups_ids)\n",
    "  combined = sorted(zip(combined_ind, combined_cat), key=lambda x: x[0])\n",
    "  combined = np.array([x[1] for x in combined])\n",
    "\n",
    "  models = {\n",
    "    'only_pressure' : only_pressure_cluster_model,\n",
    "    'wo_pressure' : wo_pressure_cluster_model\n",
    "  }\n",
    "\n",
    "  return np_utils.to_categorical(combined), models\n",
    "\n",
    "def preprocesspayloadfeaturesfulldataset(payloads, flags):\n",
    "\n",
    "\n",
    "  print('Imputing payload - keeping old value')\n",
    "  \n",
    "  print('pressure measurement')\n",
    "  pressures_f = payloads[:,-1]\n",
    "  pressure_not_nans = np.where(~np.isnan(pressures_f))[0]\n",
    "  imputebykeepinglastvalue(pressures_f, pressure_not_nans)\n",
    "\n",
    "  print('binary payload')\n",
    "  binary_f = payloads[:,-4:-1]\n",
    "  binary_f_not_nans = np.where(~np.isnan(binary_f)[:,0])[0]\n",
    "  imputebykeepinglastvalue(binary_f, binary_f_not_nans)\n",
    "\n",
    "  print('system mode payload')\n",
    "  system_f = payloads[:,-5]\n",
    "  system_f_not_nans = np.where(~np.isnan(system_f))[0]\n",
    "  imputebykeepinglastvalue(system_f, system_f_not_nans)\n",
    "\n",
    "  print('real valued payload')\n",
    "  real_val_payloads_f = payloads[:,:-5]\n",
    "  real_val_payloads_f_not_nans = np.where(~np.isnan(real_val_payloads_f)[:,0])[0]\n",
    "  imputebykeepinglastvalue(real_val_payloads_f, real_val_payloads_f_not_nans)\n",
    "\n",
    "  '''restack'''\n",
    "  payloads = np.column_stack((\n",
    "    real_val_payloads_f,\n",
    "    system_f,\n",
    "    binary_f,\n",
    "    pressures_f\n",
    "  ))\n",
    "  return payloads\n",
    "\n",
    "\n",
    "def preprocessdatafulldataset(Xs, flags):\n",
    "  \n",
    "  addresses = Xs[:, 0].reshape((-1, 1))\n",
    "  responses = Xs[:, 2].reshape((-1, 1))\n",
    "\n",
    "  functions = Xs[:, 1].reshape((-1, 1))\n",
    "  payloads = Xs[:, 3:14]\n",
    "  \n",
    "  models = None\n",
    "\n",
    "  payloads  = preprocesspayloadfeaturesfulldataset(payloads, flags)\n",
    "\n",
    "  stacked = np.column_stack((\n",
    "    addresses, \n",
    "    functions, \n",
    "    responses,\n",
    "    payloads\n",
    "  ))\n",
    "\n",
    "  if Xs.shape[1] >= 14:\n",
    "    print('Additional features were found!')\n",
    "    remaining = Xs[:, 14:]\n",
    "    stacked = np.column_stack((stacked, remaining))\n",
    "\n",
    "  return stacked\n",
    "\n",
    "def preprocesspayloadfeatures(payloads, flags, train_data_model):\n",
    "\n",
    "  if flags.payload_indicator_imputed:\n",
    "    print('Imputing payload using indicators')\n",
    "    indicators = np.isnan(payloads).astype(int)\n",
    "\n",
    "\n",
    "    '''split'''\n",
    "    realval_payload_f = payloads[:,:-5]\n",
    "    pressure_f = payloads[:,-1].reshape((-1, 1))\n",
    "\n",
    "    system_f = payloads[:,-5].reshape((-1, 1))\n",
    "    binary_payload_f = payloads[:,-4:-1]\n",
    "\n",
    "    '''categorize system_f feature'''\n",
    "    system_f = np.ma.array(system_f, mask=np.isnan(system_f)) \n",
    "    system_f = tocategoricalwithnans(system_f, 3)\n",
    "\n",
    "    '''masked normalization of realval payload features'''\n",
    "    op = 'm-' + flags.normalize\n",
    "    realval_payload_f = np.ma.array(realval_payload_f, mask=np.isnan(realval_payload_f)) \n",
    "    realval_payload_f, model = normalize(realval_payload_f, op, train_data_model)\n",
    "    realval_payload_f = np.array(realval_payload_f)\n",
    "\n",
    "    '''re-stack'''\n",
    "    payloads = np.column_stack((\n",
    "      realval_payload_f,\n",
    "      system_f,\n",
    "      binary_payload_f,\n",
    "      pressure_f\n",
    "    ))\n",
    "\n",
    "    '''replace remaining NaNs with 0'''\n",
    "    payloads[np.isnan(payloads)] = 0\n",
    "    return np.column_stack((payloads, indicators)), model\n",
    "\n",
    "  elif flags.payload_keep_value_imputed:\n",
    "    print('Imputing payload features by keeping old value')\n",
    "    \n",
    "    print('pressure measurement')\n",
    "    pressures_f = payloads[:,-1]\n",
    "\n",
    "    print('binary payload')\n",
    "    binary_f = payloads[:,-4:-1]\n",
    "\n",
    "    print('system mode payload')\n",
    "    system_f = payloads[:,-5]\n",
    "\n",
    "    print('real valued payload')\n",
    "    real_val_payloads_f = payloads[:,:-5]\n",
    "\n",
    "    op = flags.normalize\n",
    "    real_val_payloads_f, model = normalize(real_val_payloads_f, op, train_data_model)\n",
    "\n",
    "    system_f = tocategoricalwithnans(system_f, 3)\n",
    "\n",
    "    '''restack'''\n",
    "    payloads = np.column_stack((\n",
    "      real_val_payloads_f,\n",
    "      system_f,\n",
    "      binary_f,\n",
    "      pressures_f\n",
    "    ))\n",
    "    return payloads, model\n",
    "\n",
    "  else:\n",
    "    return clusterpayloadfeatures(payloads, flags, train_data_model)\n",
    "\n",
    "def imputebykeepinglastvalue(features, not_nans):\n",
    "  first_not_nan = not_nans[0]\n",
    "  features[:first_not_nan] = features[first_not_nan]\n",
    "\n",
    "  for begin, end in pairwise(not_nans):\n",
    "    features[begin:end] = features[begin]\n",
    "\n",
    "  '''\n",
    "  Case: To keep the value until the end of dataset\n",
    "  '''\n",
    "  last = len(features)\n",
    "  last_not_nan = not_nans[-1]\n",
    "  if last != last_not_nan:\n",
    "    features[last_not_nan+1:] = features[last_not_nan]\n",
    "\n",
    "def grouppayloads(payloads):\n",
    "  '''\n",
    "  Last two columns uniquely identify a category of payload\n",
    "    both columns are NaNs -> 0\n",
    "    first is NaN and second is not NaN -> 1\n",
    "    first is not NaN and second is NaN -> 2\n",
    "  Returns indicies\n",
    "  '''\n",
    "  ids = payloads[:,-2:]\n",
    "  ids = np.packbits(~np.isnan(ids), axis=1) // 64\n",
    "  return ids.reshape(-1)\n",
    "\n",
    "def preprocessfunctioncodes(functions, flags, train_data_model):\n",
    "\n",
    "  if flags.encode_function:    \n",
    "    '''\n",
    "    Encode function - same for training and validation/testing data\n",
    "    '''\n",
    "    print('Encoding function...')\n",
    "    encoder = LabelEncoder()\n",
    "    functions = encoder.fit_transform(functions.reshape(-1))\n",
    "    return np_utils.to_categorical(functions), None\n",
    "\n",
    "  if not train_data_model:\n",
    "\n",
    "    if flags.cluster_function_kmeans != None:\n",
    "      k = flags.cluster_function_kmeans\n",
    "      predicted, model = clusterkmeans(functions, 'function', elbow=flags.elbow, k=k)\n",
    "      return np_utils.to_categorical(predicted), model\n",
    "\n",
    "    elif flags.cluster_function_gmm:\n",
    "      k = flags.cluster_function_gmm\n",
    "      predicted, model = clustergmm(functions, 'function', elbow=flags.elbow, k=k)\n",
    "      return np_utils.to_categorical(predicted), model\n",
    "\n",
    "  else:\n",
    "    return np_utils.to_categorical(train_data_model.predict(functions)), None\n",
    "\n",
    "def preprocessdata(Xs, flags, train_data_models = None):\n",
    "  \n",
    "  addresses = Xs[:, 0].reshape((-1, 1))\n",
    "  responses = Xs[:, 2].reshape((-1, 1))\n",
    "\n",
    "  functions = Xs[:, 1].reshape((-1, 1))\n",
    "  payloads = Xs[:, 3:14]\n",
    "  \n",
    "  models = None\n",
    "\n",
    "  if not train_data_models:\n",
    "    functions, f_model = preprocessfunctioncodes(functions, flags, None)\n",
    "    payloads, p_model  = preprocesspayloadfeatures(payloads, flags, None)\n",
    "    models = {'function': f_model, 'payload': p_model}\n",
    "\n",
    "  else:\n",
    "    f_model = train_data_models['function']\n",
    "    p_model = train_data_models['payload']\n",
    "\n",
    "    functions, _ = preprocessfunctioncodes(functions, flags, f_model)\n",
    "    payloads, _  = preprocesspayloadfeatures(payloads, flags, p_model)  \n",
    "\n",
    "  stacked = np.column_stack((\n",
    "    addresses, \n",
    "    functions, \n",
    "    responses,\n",
    "    payloads\n",
    "  ))\n",
    "\n",
    "  if Xs.shape[1] >= 14:\n",
    "    print('Additional features were found!')\n",
    "    remaining = Xs[:, 14:]\n",
    "\n",
    "    if not train_data_models:\n",
    "      remaining, r_model = normalize(remaining, flags.normalize, None)\n",
    "\n",
    "      models['remaining'] = r_model\n",
    "    else:\n",
    "      r_model = train_data_models['remaining']\n",
    "      remaining, _ = normalize(remaining, flags.normalize, r_model)\n",
    "\n",
    "    stacked = np.column_stack((stacked, remaining))\n",
    "\n",
    "  return stacked, models\n",
    "\n",
    "def cluster(model_class, score_func, Xs, feature_name, k, elbow, max_k):\n",
    "  \n",
    "  if elbow:\n",
    "    models = [model_class(i).fit(Xs) for i in range(1, max_k+1)]\n",
    "    scores = [score_func(model, Xs) for model in models]\n",
    "    k = promptelbowmethod(scores, feature_name)\n",
    "\n",
    "  m = model_class(k).fit(Xs)\n",
    "  return m.predict(Xs), m\n",
    "\n",
    "def clustergmm(Xs, feature_name, k=None, elbow=True):\n",
    "  \n",
    "  print('Clustering {} with GMM'.format(feature_name))\n",
    "  GMM = lambda k: GaussianMixture(n_components=k, init_params='random')\n",
    "  score_func = lambda gmm, Xs: gmm.aic(Xs)\n",
    "  return cluster(GMM, score_func, Xs, feature_name, k, elbow, max_k=20)\n",
    "\n",
    "def clusterkmeans(Xs, feature_name, k=None, elbow=True):\n",
    "  \n",
    "  print('Clustering {} with Kmeans'.format(feature_name))\n",
    "  KM = lambda k: KMeans(n_clusters=k)\n",
    "  score_func = lambda km, Xs: km.score(Xs)\n",
    "  return cluster(KM, score_func, Xs, feature_name, k, elbow, max_k=10)\n",
    "\n",
    "def promptelbowmethod(scores, feature_name):\n",
    "  print(scores)\n",
    "  xs = range(1, len(scores)+1)\n",
    "  ys = np.array(scores)\n",
    "  is_log = ''\n",
    "  if np.all(ys > 0) or np.all(ys < 0):\n",
    "    ys = np.log(np.abs(ys))\n",
    "    is_log = 'log'\n",
    "  print(ys)\n",
    "  pl.plot(xs, ys)\n",
    "  pl.xticks(xs)\n",
    "  pl.xlabel('Number of clusters:')\n",
    "  pl.ylabel('{} Score: '.format(is_log))\n",
    "  pl.title('Elbow method - {}'.format(feature_name))\n",
    "  pl.show()\n",
    "  print('Enter number of clusters- {}:'.format(feature_name))\n",
    "  k = int(input('-->'))\n",
    "  print('\\nselected k: {}'.format(k))\n",
    "  return k\n",
    "\n",
    "#-----------------------------------#\n",
    "# Run Data Pre-processing Main Code #\n",
    "#-----------------------------------#\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser(description='Preprocess SCADA dataset.')\n",
    "  parser.add_argument('-d', '--dataset', type=str, help='Dataset filename')\n",
    "  parser.add_argument('--label', type=str, choices=['binary', 'category', 'dcategory'],\n",
    "    default='binary', help='Type of labels to output in the preprocessed dataset')\n",
    "  parser.add_argument('-t', '--time-series', action='store_true', \n",
    "    help='Keep temporal structure of the data')\n",
    "  parser.add_argument('-n', '--normalize', type=str, choices=['mean', 'minmax'],\n",
    "    default='mean', help='Type of normalization to preform')\n",
    "  parser.add_argument('--discard-crc', action='store_true', \n",
    "    help='Remove CRC feature from the dataset')\n",
    "  parser.add_argument('--discard-length', action='store_true', \n",
    "    help='Remove Length feature from the dataset')\n",
    "\n",
    "  function_encoding_group = parser.add_mutually_exclusive_group(required=True)\n",
    "  function_encoding_group.add_argument('--encode-function', action='store_true',\n",
    "    help='Encode function codes with One-Hot encoding')\n",
    "  function_encoding_group.add_argument('--cluster-function-kmeans', type=int,\n",
    "    help='Cluster function codes with kmeans')\n",
    "  function_encoding_group.add_argument('--cluster-function-gmm', type=int,\n",
    "    help='Cluster function codes with GMM')\n",
    "\n",
    "  payload_features_impution_group = parser.add_mutually_exclusive_group(required=True)\n",
    "  payload_features_impution_group.add_argument('--payload-keep-value-imputed', action='store_true',\n",
    "    help='Impute payload features by keeping the oldest value')\n",
    "  payload_features_impution_group.add_argument('--payload-indicator-imputed', action='store_true',\n",
    "    help='Impute payload features with 0\\'s and add indicators')    \n",
    "  payload_features_impution_group.add_argument('--payload-kmeans-imputed', type=int, nargs=2,\n",
    "    help='Impute payload features using clustering with kmeans. Starting with pressure then remaining')\n",
    "  payload_features_impution_group.add_argument('--payload-gmm-imputed', type=int, nargs=2,\n",
    "    help='Impute payload features using clustering with GMM. Starting with pressure then remaining')    \n",
    "\n",
    "  parser.add_argument('--elbow', action='store_true', \n",
    "    help='Clustering with elbow method for best number of clusters')\n",
    "\n",
    "  parser.add_argument('-o', '--output', type=str, help='Output .npy files')\n",
    "  parser.add_argument('--split', type=float, nargs=3, help='train/val/test split ratio')\n",
    "\n",
    "  flags = parser.parse_args('-d ./dataset/IanArffDataset.arff --split 0.6 0.2 0.2 --encode-function --label binary --time-series -n mean --payload-indicator-imputed  -o processeddataset'.split())\n",
    "\n",
    "  dataset, meta = arff.loadarff(flags.dataset)\n",
    "\n",
    "  label_types = {\n",
    "    'binary'    : 'binary result',\n",
    "    'category'  : 'categorized result',\n",
    "    'dcategory' : 'specific result'\n",
    "  }\n",
    "\n",
    "  print('____________________________________________')\n",
    "  print('START Data Pre-processing')\n",
    "  print('____________________________________________')\n",
    "  print('Encoding labels...')\n",
    "  label_name = label_types[flags.label]\n",
    "  labels = dataset[label_name].astype(np.float)\n",
    "  labels = labels.reshape((-1, 1))\n",
    "  \n",
    "  \n",
    "  print('Encoding addressess...')\n",
    "  #pre-process address before splitting, simple change of values and reshape into column vec\n",
    "  addresses = preprocessing.label_binarize(dataset['address'], classes=[4])\n",
    "\n",
    "  print('Encoding functions...')\n",
    "  functions = dataset['function'].astype(np.float).reshape((-1, 1))\n",
    "\n",
    "  print('Encoding command responses...')\n",
    "  #pre-process address before splitting, parse string and reshape into column vec\n",
    "  responses = dataset['command response'].astype(np.float).reshape((-1, 1))\n",
    "\n",
    "  print('Extracting payload features...')\n",
    "  payload_feature_names = meta.names()[3:14]\n",
    "    \n",
    "  payload_features = repack_fields(dataset[payload_feature_names])\n",
    "  print(payload_features.dtype)\n",
    "  payload_features = payload_features \\\n",
    "    .view(np.float64) \\\n",
    "    .reshape(payload_features.shape + (-1,))\n",
    "    \n",
    "\n",
    "  Xs = np.column_stack((\n",
    "    addresses, \n",
    "    functions, \n",
    "    responses,\n",
    "    payload_features\n",
    "  ))\n",
    "\n",
    "  if not flags.discard_length:\n",
    "    print('\\tAdding length...')\n",
    "    lengths = dataset['length'].astype(np.float).reshape((-1, 1))\n",
    "    Xs = np.column_stack((Xs, lengths))\n",
    "\n",
    "  if not flags.discard_crc:\n",
    "    print('\\tAdding crc rate...')\n",
    "    crcs = dataset['crc rate'].astype(np.float).reshape((-1, 1))\n",
    "    Xs = np.column_stack((Xs, crcs))\n",
    "\n",
    "  if flags.time_series:\n",
    "    print('\\tAdding timestamp differences...')\n",
    "    Xs = np.column_stack((Xs, dataset['time']))\n",
    "    \n",
    "\n",
    "  if flags.payload_keep_value_imputed:\n",
    "\n",
    "    Xs = preprocessdatafulldataset(Xs,flags)\n",
    "    Xs, labels = sklearn.utils.shuffle(Xs, labels)\n",
    "\n",
    "    print('\\n\\tSplitting dataset...')\n",
    "    Xs, Ys = splitdataset(Xs, labels, *flags.split)\n",
    "    Xs_train, Xs_val, Xs_test = Xs\n",
    "    Ys_train, Ys_val, Ys_test = Ys\n",
    "\n",
    "    Xs_train_val = np.concatenate((Xs_train,Xs_val))\n",
    "    Ys_train_val = np.concatenate((Ys_train,Ys_val))\n",
    "\n",
    "    Xs_train_test = Xs_test\n",
    "    Ys_train_test = Ys_test\n",
    "\n",
    "    Xs_train, train_models = preprocessdata(Xs_train, flags)\n",
    "    Xs_train_val, train_models_val = preprocessdata(Xs_train_val, flags)\n",
    "    Xs_val,  _ = preprocessdata(Xs_val,  flags, train_data_models=train_models)\n",
    "    Xs_test, _ = preprocessdata(Xs_test, flags, train_data_models=train_models)\n",
    "    Xs_train_test, _ = preprocessdata(Xs_train_test, flags, train_data_models=train_models_val)\n",
    "\n",
    "  else:\n",
    "    print('\\n\\tSplitting dataset...')\n",
    "    Xs, labels = sklearn.utils.shuffle(Xs, labels)\n",
    "    Xs, Ys = splitdataset(Xs, labels, *flags.split)\n",
    "    Xs_train, Xs_val, Xs_test = Xs\n",
    "    Ys_train, Ys_val, Ys_test = Ys\n",
    "\n",
    "    Xs_train_val = np.concatenate((Xs_train,Xs_val))\n",
    "    Ys_train_val = np.concatenate((Ys_train,Ys_val))\n",
    "    Xs_train_test = Xs_test\n",
    "    Ys_train_test = Ys_test\n",
    "\n",
    "    Xs_train, train_models = preprocessdata(Xs_train, flags)\n",
    "    Xs_val,  _ = preprocessdata(Xs_val,  flags, train_data_models=train_models)\n",
    "    Xs_test, _ = preprocessdata(Xs_test, flags, train_data_models=train_models)\n",
    "\n",
    "    Xs_train_val, train_models_val = preprocessdata(Xs_train_val, flags)\n",
    "    Xs_train_test, _ = preprocessdata(Xs_train_test, flags, train_data_models=train_models_val)\n",
    "\n",
    "    print(Xs_train_val.shape, Ys_train_val.shape)\n",
    "    print(Xs_train_test.shape, Ys_train_test.shape)\n",
    "\n",
    "\n",
    "  if not flags.time_series:\n",
    "    Xs_train, Ys_train = sklearn.utils.shuffle(Xs_train, Ys_train, random_state=0)\n",
    " \n",
    "  output_dir = flags.output\n",
    "\n",
    "  if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "  np.save(os.path.join(output_dir, 'Xs_train'), Xs_train)\n",
    "  np.save(os.path.join(output_dir, 'Xs_val'),   Xs_val)\n",
    "  np.save(os.path.join(output_dir, 'Xs_test'),  Xs_test)\n",
    "\n",
    "  np.save(os.path.join(output_dir, 'Ys_train'), Ys_train)\n",
    "  np.save(os.path.join(output_dir, 'Ys_val'),   Ys_val)\n",
    "  np.save(os.path.join(output_dir, 'Ys_test'),  Ys_test)\n",
    "\n",
    "  np.save(os.path.join(output_dir, 'Xs_train_val'), Xs_train_val)\n",
    "  np.save(os.path.join(output_dir, 'Xs_train_test'), Xs_train_test)\n",
    "  np.save(os.path.join(output_dir, 'Ys_train_val'), Ys_train_val)\n",
    "  np.save(os.path.join(output_dir, 'Ys_train_test'), Ys_train_test)\n",
    "    \n",
    "  print('____________________________________________')\n",
    "  print('END Data Pre-processing')\n",
    "  print('____________________________________________')\n",
    "    \n",
    "    \n",
    "#-----------------------#  \n",
    "# PREDICTION MODEL CODE #\n",
    "#-----------------------#\n",
    "\n",
    "  print('____________________________________________')\n",
    "  print('START Model Run')\n",
    "  print('____________________________________________')\n",
    "\n",
    "def main():\n",
    "  start_time = time.time()\n",
    "  parser = argparse.ArgumentParser(description='Run RF:')\n",
    "  parser.add_argument('-d', '--dataset', required=True, type=str, help='Data directory')\n",
    "  parser.add_argument('-i', '--iters', default=10, type=int, help='Number of random samples')\n",
    "\n",
    "  flags = parser.parse_args('-d ./processeddataset/ --i 2'.split())\n",
    "\n",
    "  dataset_dir = flags.dataset\n",
    "  dataset_filenames = [\n",
    "    'Xs_train.npy', 'Xs_val.npy', 'Xs_test.npy', 'Xs_train_val.npy', 'Xs_train_test.npy',\n",
    "    'Ys_train.npy', 'Ys_val.npy', 'Ys_test.npy', 'Ys_train_val.npy', 'Ys_train_test.npy'\n",
    "  ]\n",
    "\n",
    "  dataset_filenames = map(lambda x: os.path.join(dataset_dir, x), dataset_filenames)\n",
    "  X_train, X_valid, X_test, Xs_train_val,Xs_train_test, Y_train, Y_valid, Y_test, Ys_train_val,Ys_train_test = map(np.load, dataset_filenames)\n",
    "  \n",
    "  n_label, label_dim = Y_train.shape\n",
    "  print(np.unique(Y_train))\n",
    "  print(label_dim)\n",
    "\n",
    "  output_dir = os.path.join(dataset_dir, 'results_hyperparameters')\n",
    "  if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "  '''\n",
    "  if len(np.unique(Y_train)) > 8:\n",
    "    print(\"More than 8 labels\")\n",
    "    category = {0:0, 1:4, 2:4, 3:4, 4:4, 5:4, 6:4, 7:4, 8:4, 9:4, 10:4, 11:4, 12:4,\n",
    "                13:3, 14:3, 15:3, 16:3, 17:3, 18:6, 19:5, 20:7, 21:5, 22:5, 23:7, 24:7,\n",
    "                25:2, 26:2, 27:2, 28:2, 29:1, 30:1, 31:1, 32:1, 33:2, 34:2, 35:2}\n",
    "\n",
    "    for i in range(Y_train.shape[0]):\n",
    "      key = Y_train[i]\n",
    "      Y_train[i] = category.get(key[0])\n",
    "    for j in range(Y_valid.shape[0]):\n",
    "      key = Y_valid[j]\n",
    "      Y_valid[j] = category.get(key[0])\n",
    "    for k in range(Y_test.shape[0]):\n",
    "      key = Y_test[k]\n",
    "      Y_test[k] = category.get(key[0])\n",
    "  '''\n",
    "\n",
    "  print('Label dimension is: {}'.format(label_dim))\n",
    "  \n",
    "  if not label_dim in [1, 2, 8, 36]:\n",
    "    raise Exception('!Unknown label dimension: {}'.format(label_dim))\n",
    "\n",
    "\n",
    "\n",
    "  for iterations in range(flags.iters):\n",
    "    input_dim = X_train.shape[0]\n",
    "    input_val = X_valid.shape[0]\n",
    "    input_test= X_test.shape[0]\n",
    "    print('Input dimension - training data: {}'.format(input_dim))\n",
    "    print('Input dimension - validation data: {}'.format(input_val))\n",
    "    print('Input dimension - test data: {}'.format(input_test))\n",
    "    \n",
    "\n",
    "    ne =  np.random.randint(2,100)\n",
    "    md   = np.random.randint(2,100)\n",
    "\n",
    "    cr = np.random.choice(['gini'])\n",
    "    cw = 'balanced'\n",
    "    #bo = False -> Keep strategy & bo = True -> rest of the strategies\n",
    "    bo = np.random.choice([False])\n",
    "    #mf = 0.5 -> Keep strategy & mf = -> rest of the strategies\n",
    "    mf = 0.5\n",
    "    ws = False\n",
    "\n",
    "    print('n_estimators: '+str(ne))\n",
    "    print('criterion: '+str(cr))\n",
    "    print('max_depth: '+str(md))\n",
    "    print('bootstrap: '+str(bo))\n",
    "    print('max max_features: '+str(mf))\n",
    "\n",
    "\n",
    "    hyperparams = str(ne)+'--'+str(cr)+'--'+str(md)+'--'+str(bo)+'--balanced.txt'\n",
    "    output_file = os.path.join(output_dir, hyperparams)\n",
    "    with open(output_file, 'w') as f:\n",
    "      f.write('Label dimension: {}'.format(label_dim)+\"\\n\")\n",
    "      f.write('Input dimension: {}'.format(input_dim)+\"\\n\")\n",
    "\n",
    "\n",
    "      clf = RandomForestClassifier(bootstrap=bo, class_weight= cw, criterion=cr,\n",
    "            max_depth=md, max_features=mf, max_leaf_nodes=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=ne, n_jobs=-1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=ws)\n",
    "\n",
    "      clf.fit(X_train, Y_train.ravel())\n",
    "\n",
    "      predictions_train = clf.predict(X_train)  \n",
    "      acc_train = accuracy_score(Y_train,predictions_train)\n",
    "        \n",
    "      #--------------------#\n",
    "      # Curve code - START #\n",
    "      #--------------------#\n",
    "    \n",
    "      #-----------------------#\n",
    "      # Visualizations & More #\n",
    "      #-----------------------#\n",
    "\n",
    "#       X_TRAIN_SHAPE = X_train.shape\n",
    "#       print('X_TRAIN_SHAPE:'+str(X_TRAIN_SHAPE))\n",
    "      \n",
    "#       Y_TRAIN_SHAPE = Y_train.shape\n",
    "#       print('Y_TRAIN_SHAPE:'+str(Y_TRAIN_SHAPE))\n",
    "        \n",
    "#       X_train = X_train.reshape(X_train.shape[:1])\n",
    "#       X_train = X_train.transpose()\n",
    "        \n",
    "#       Y_train = Y_train.reshape(Y_train.shape[:1])\n",
    "#       Y_train = Y_train.transpose()\n",
    "        \n",
    "#       X_TRAIN_SHAPE = X_train.shape\n",
    "#       print('X_TRAIN_SHAPE:'+str(X_TRAIN_SHAPE))\n",
    "      \n",
    "#       Y_TRAIN_SHAPE = Y_train.shape\n",
    "#       print('Y_TRAIN_SHAPE:'+str(Y_TRAIN_SHAPE))\n",
    "    \n",
    "#       print('____________________________________________')\n",
    "#       print('PLOT: Actual vs Predicted')\n",
    "#       print('____________________________________________')\n",
    "    \n",
    "#       print('CHART:')\n",
    "\n",
    "#       plt.plot(predictions_train,color='red',label='predicted')\n",
    "#       plt.plot(Y_test,color='purple',label=\"actual\")\n",
    "#       plt.xlabel(\"xlabel\")\n",
    "#       plt.ylabel(\"ylabel\")\n",
    "#       leg = plt.legend()\n",
    "#       plt.show()\n",
    "        \n",
    "#       print('SCATTER PLOT:')\n",
    "        \n",
    "#       plt.scatter(X_train, Y_train, color = 'red') \n",
    "#       plt.plot(X_train, predictions_train, color = 'blue') \n",
    "#       plt.title('Actual vs Predicted on Training set')\n",
    "#       plt.xlabel('X')\n",
    "#       plt.ylabel('Y')\n",
    "#       plt.show()\n",
    "        \n",
    "#       plt.figure(figsize=(10,10))\n",
    "#       plt.scatter(X_train, Y_train, c='crimson')\n",
    "#       plt.yscale('log')\n",
    "#       plt.xscale('log')\n",
    "\n",
    "#       p1 = max(max(input_test), max(input_dim))\n",
    "#       p2 = min(min(input_test), min(input_dim))\n",
    "#       plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "#       plt.xlabel('True Values', fontsize=15)\n",
    "#       plt.ylabel('Predictions', fontsize=15)\n",
    "#       plt.axis('equal')\n",
    "#       plt.show()\n",
    "\n",
    "    #--------------------#\n",
    "    # Curve code - END #\n",
    "    #--------------------#\n",
    "        \n",
    "      predictions_val = clf.predict(X_valid)  \n",
    "      acc_val = accuracy_score(Y_valid,predictions_val)\n",
    "        \n",
    "      classiReport = classification_report(Y_valid, predictions_val,digits=4)\n",
    "\n",
    "      f.write('max_features: {}'.format(mf)+\"\\n\")\n",
    "      print(\"Accuracy on trainning data = \"+str(acc_train))\n",
    "      f.write(\"Accuray on training data = \"+str(acc_train)+\"\\n\")\n",
    "      print(\"Accuray on validation data = \"+str(acc_val))\n",
    "      f.write(\"Accuray on validation data = \"+str(acc_val)+\"\\n\")\n",
    "      print(classiReport)\n",
    "      f.write(classiReport+\"\\n\")\n",
    "\n",
    "      print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "      f.write(\"--- %s seconds ---\" % (time.time() - start_time)+\"\\n\")\n",
    "        \n",
    "      #-----------------------#\n",
    "      # Visualizations & More #\n",
    "      #-----------------------#\n",
    "\n",
    "#       print('____________________________________________')\n",
    "#       print('VISUALIZATIONS & MORE')\n",
    "#       print('____________________________________________')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
