{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________\n",
      "START Data Pre-processing\n",
      "____________________________________________\n",
      "Encoding labels...\n",
      "Encoding addressess...\n",
      "Encoding functions...\n",
      "Encoding command responses...\n",
      "Extracting payload features\n",
      "\tAdding length...\n",
      "\tAdding crc rate...\n",
      "\tAdding timestamp differences...\n",
      "\n",
      "\tSplitting dataset...\n",
      "Encoding function...\n",
      "Imputing payload features using indicators\n",
      "Additional features were found!\n",
      "Encoding function...\n",
      "Imputing payload features using indicators\n",
      "Additional features were found!\n",
      "Encoding function...\n",
      "Imputing payload features using indicators\n",
      "Additional features were found!\n",
      "Encoding function...\n",
      "Imputing payload features using indicators\n",
      "Additional features were found!\n",
      "Encoding function...\n",
      "Imputing payload features using indicators\n",
      "Additional features were found!\n",
      "(219701, 57) (219701, 1)\n",
      "(54927, 57) (54927, 1)\n",
      "____________________________________________\n",
      "END Data Pre-processing\n",
      "____________________________________________\n",
      "____________________________________________\n",
      "START Building Model\n",
      "____________________________________________\n",
      "(219701, 57) (219701, 1)\n",
      "(54925, 57) (54925, 1)\n",
      "(54927, 57) (54927, 1)\n",
      "Label dimension: 1\n",
      "Input dimension: 57\n",
      "Number of iterations: 2\n",
      "Hyperparameters setting:\n",
      "0.038114720218641755,132,1,4,0.1675711911800153,98,4\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 4, 114)            52440     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4, 114)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 4, 1)              115       \n",
      "=================================================================\n",
      "Total params: 52,555\n",
      "Trainable params: 52,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Preparing sequences - Length = 4\n",
      "____________________________________________\n",
      "MODEL EVALUATION\n",
      "____________________________________________\n",
      "(54925, 4, 57) (54925, 4, 1)\n",
      "(13731, 4, 57) (13731, 4, 1)\n",
      "(13731, 4, 57) (13731, 4, 1)\n",
      "  1/417 [..............................] - ETA: 26:11 - loss: 0.7674 - acc: 0.3182"
     ]
    }
   ],
   "source": [
    "#--------------------#\n",
    "# Import statements #\n",
    "#--------------------#\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.recfunctions import repack_fields\n",
    "\n",
    "import scipy, argparse, itertools, os\n",
    "from scipy.io import arff\n",
    "\n",
    "import sklearn.utils\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Output\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Visualizations\n",
    "import matplotlib.pyplot as mp\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly as pl\n",
    "import plotly.offline as plo\n",
    "import plotly.graph_objs as plg\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "#Prediction Model\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense, Dropout,Activation, Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "import tensorflow.keras.callbacks\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "#--------------------#\n",
    "# Classes & Methods #\n",
    "#--------------------#\n",
    "\n",
    "#Class MaskedStandardScaler\n",
    "class MSS:\n",
    "\n",
    "  def fit_transform(self, Xs):\n",
    "    self.mean = Xs.mean(axis=0)\n",
    "    self.std = Xs.std(axis=0)\n",
    "    return self.transform(Xs)\n",
    "\n",
    "  def transform(self, Xs):\n",
    "    return (Xs - self.mean) / self.std\n",
    "\n",
    "#Class MaskedMinMaxScaler\n",
    "class MMMS:\n",
    "\n",
    "  def fit_transform(self, Xs):\n",
    "    self.min = Xs.min(axis=0)\n",
    "    self.max = Xs.max(axis=0)\n",
    "    return self.transform(Xs)\n",
    "\n",
    "  def transform(self, Xs):\n",
    "    return (Xs - self.min) / (self.max - self.min)\n",
    "\n",
    "def tocategoricalwithnans(Xs, n_cats):\n",
    "  categorized = np.zeros((len(Xs), n_cats))\n",
    "  for n, v in enumerate(np.array(Xs)):\n",
    "    if not np.isnan(v):\n",
    "      categorized[n, int(v)] = 1 \n",
    "  return categorized\n",
    "\n",
    "def pairwise(iterable):\n",
    "  a, b = itertools.tee(iterable)\n",
    "  next(b, None)\n",
    "  return zip(a, b)\n",
    "\n",
    "def splitdataset(data, labels, train_per_split, val_per_split, test_per_split):\n",
    "  assert (train_per_split + val_per_split + test_per_split) <= 1.0\n",
    "  \n",
    "  train_end_index = int(data.shape[0] * train_per_split)\n",
    "  valid_end_index = int(data.shape[0] * val_per_split) + train_end_index\n",
    "\n",
    "  X_train, Y_train = data[:train_end_index], labels[:train_end_index]\n",
    "  X_valid, Y_valid = data[train_end_index:valid_end_index], labels[train_end_index:valid_end_index]\n",
    "  X_test,  Y_test  = data[valid_end_index:], labels[valid_end_index:]\n",
    "  \n",
    "  return (X_train, X_valid, X_test), (Y_train, Y_valid, Y_test)\n",
    "\n",
    "def normalize(Xs, op, train_data_model):\n",
    "\n",
    "  ops = { \n",
    "    'mean'     : preprocessing.StandardScaler,\n",
    "    'minmax'   : preprocessing.MinMaxScaler,\n",
    "    'm-mean'   : MSS,\n",
    "    'm-minmax' : MMMS\n",
    "  }\n",
    "\n",
    "  if not train_data_model:\n",
    "    model = ops[op]()\n",
    "    Xs = model.fit_transform(Xs)\n",
    "    return Xs, model\n",
    "  else:\n",
    "    return train_data_model.transform(Xs), None\n",
    "\n",
    "def getclustermodelk(model, flags):\n",
    "  if flags.payload_kmeans_imputed:\n",
    "    return model.cluster_centers_.shape[0]\n",
    "  elif flags.payload_gmm_imputed:\n",
    "    return model.weights_.shape[0]\n",
    "\n",
    "def clusterpayloadfeatures(payloads, flags, train_data_model):\n",
    "  groups = grouppayloads(payloads)\n",
    "  groups_ids = [np.where(groups == i)[0] for i in range(3)]\n",
    "\n",
    "  only_nans = np.zeros_like(groups_ids[0])\n",
    "  only_pres = payloads[groups_ids[1]][:,-1].reshape((-1, 1))\n",
    "  wo_pres   = payloads[groups_ids[2]][:,:-1]\n",
    "\n",
    "  if not train_data_model:\n",
    "    \n",
    "    if flags.payload_kmeans_imputed:\n",
    "      print('Imputing payload with kmeans')\n",
    "      k1, k2 = flags.payload_kmeans_imputed\n",
    "      cluster_func = clusterkmeans\n",
    "    elif flags.payload_gmm_imputed:\n",
    "      print('Imputing payload with GMM')\n",
    "      k1, k2 = flags.payload_gmm_imputed\n",
    "      cluster_func = clustergmm\n",
    "\n",
    "    only_pres, only_pressure_cluster_model = cluster_func(only_pres, \\\n",
    "      'pressure', elbow=flags.elbow, k=k1)\n",
    "    wo_pres, wo_pressure_cluster_model = cluster_func(wo_pres, \\\n",
    "      'remaining payload features', elbow=flags.elbow, k=k2)\n",
    "\n",
    "  else:\n",
    "    only_pressure_cluster_model = train_data_model['only_pressure']\n",
    "    wo_pressure_cluster_model = train_data_model['wo_pressure']\n",
    "    only_pres = only_pressure_cluster_model.predict(only_pres)\n",
    "    wo_pres = wo_pressure_cluster_model.predict(wo_pres)\n",
    "\n",
    "  only_pres += 1\n",
    "  wo_pres += (getclustermodelk(only_pressure_cluster_model, flags) + 1)\n",
    "\n",
    "  combined_cat = np.concatenate((only_nans, only_pres, wo_pres))\n",
    "  combined_ind = np.concatenate(groups_ids)\n",
    "  combined = sorted(zip(combined_ind, combined_cat), key=lambda x: x[0])\n",
    "  combined = np.array([x[1] for x in combined])\n",
    "\n",
    "  models = {\n",
    "    'only_pressure' : only_pressure_cluster_model,\n",
    "    'wo_pressure' : wo_pressure_cluster_model\n",
    "  }\n",
    "\n",
    "  return np_utils.to_categorical(combined), models\n",
    "\n",
    "def preprocesspayloadfeaturesfulldataset(payloads, flags):\n",
    "\n",
    "\n",
    "  print('Imputing payload features - keeping old value')\n",
    "  \n",
    "  print('pressure measurement')\n",
    "  pressures_f = payloads[:,-1]\n",
    "  pressure_not_nans = np.where(~np.isnan(pressures_f))[0]\n",
    "  imputebykeepinglastvalue(pressures_f, pressure_not_nans)\n",
    "\n",
    "  print('binary payload')\n",
    "  binary_f = payloads[:,-4:-1]\n",
    "  binary_f_not_nans = np.where(~np.isnan(binary_f)[:,0])[0]\n",
    "  imputebykeepinglastvalue(binary_f, binary_f_not_nans)\n",
    "\n",
    "  print('system mode payload')\n",
    "  system_f = payloads[:,-5]\n",
    "  system_f_not_nans = np.where(~np.isnan(system_f))[0]\n",
    "  imputebykeepinglastvalue(system_f, system_f_not_nans)\n",
    "\n",
    "  print('real valued payload')\n",
    "  real_val_payloads_f = payloads[:,:-5]\n",
    "  real_val_payloads_f_not_nans = np.where(~np.isnan(real_val_payloads_f)[:,0])[0]\n",
    "  imputebykeepinglastvalue(real_val_payloads_f, real_val_payloads_f_not_nans)\n",
    "\n",
    "  '''restack'''\n",
    "  payloads = np.column_stack((\n",
    "    real_val_payloads_f,\n",
    "    system_f,\n",
    "    binary_f,\n",
    "    pressures_f\n",
    "  ))\n",
    "  return payloads\n",
    "\n",
    "\n",
    "def preprocessdatafulldataset(Xs, flags):\n",
    "  \n",
    "  addresses = Xs[:, 0].reshape((-1, 1))\n",
    "  responses = Xs[:, 2].reshape((-1, 1))\n",
    "\n",
    "  functions = Xs[:, 1].reshape((-1, 1))\n",
    "  payloads = Xs[:, 3:14]\n",
    "  \n",
    "  models = None\n",
    "\n",
    "  payloads  = preprocesspayloadfeaturesfulldataset(payloads, flags)\n",
    "\n",
    "  stacked = np.column_stack((\n",
    "    addresses, \n",
    "    functions, \n",
    "    responses,\n",
    "    payloads\n",
    "  ))\n",
    "\n",
    "  if Xs.shape[1] >= 14:\n",
    "    print('Additional features were found!')\n",
    "    remaining = Xs[:, 14:]\n",
    "    stacked = np.column_stack((stacked, remaining))\n",
    "\n",
    "  return stacked\n",
    "\n",
    "def preprocesspayloadfeatures(payloads, flags, train_data_model):\n",
    "\n",
    "  if flags.payload_indicator_imputed:\n",
    "    print('Imputing payload features using indicators')\n",
    "    indicators = np.isnan(payloads).astype(int)\n",
    "\n",
    "\n",
    "    '''split'''\n",
    "    realval_payload_f = payloads[:,:-5]\n",
    "    pressure_f = payloads[:,-1].reshape((-1, 1))\n",
    "\n",
    "    system_f = payloads[:,-5].reshape((-1, 1))\n",
    "    binary_payload_f = payloads[:,-4:-1]\n",
    "\n",
    "    '''categorize system_f feature'''\n",
    "    system_f = np.ma.array(system_f, mask=np.isnan(system_f)) \n",
    "    system_f = tocategoricalwithnans(system_f, 3)\n",
    "\n",
    "    '''masked normalization of realval payload features'''\n",
    "    op = 'm-' + flags.normalize\n",
    "    realval_payload_f = np.ma.array(realval_payload_f, mask=np.isnan(realval_payload_f)) \n",
    "    realval_payload_f, model = normalize(realval_payload_f, op, train_data_model)\n",
    "    realval_payload_f = np.array(realval_payload_f)\n",
    "\n",
    "    '''re-stack'''\n",
    "    payloads = np.column_stack((\n",
    "      realval_payload_f,\n",
    "      system_f,\n",
    "      binary_payload_f,\n",
    "      pressure_f\n",
    "    ))\n",
    "\n",
    "    '''replace remaining NaNs with 0'''\n",
    "    payloads[np.isnan(payloads)] = 0\n",
    "    return np.column_stack((payloads, indicators)), model\n",
    "\n",
    "  elif flags.payload_keep_value_imputed:\n",
    "    print('Imputing payload features - keeping old value')\n",
    "    \n",
    "    print('pressure measurement')\n",
    "    pressures_f = payloads[:,-1]\n",
    "\n",
    "    print('binary payload')\n",
    "    binary_f = payloads[:,-4:-1]\n",
    "\n",
    "    print('system mode payload')\n",
    "    system_f = payloads[:,-5]\n",
    "\n",
    "    print('real valued payload')\n",
    "    real_val_payloads_f = payloads[:,:-5]\n",
    "\n",
    "    op = flags.normalize\n",
    "    real_val_payloads_f, model = normalize(real_val_payloads_f, op, train_data_model)\n",
    "\n",
    "    system_f = tocategoricalwithnans(system_f, 3)\n",
    "\n",
    "    '''restack'''\n",
    "    payloads = np.column_stack((\n",
    "      real_val_payloads_f,\n",
    "      system_f,\n",
    "      binary_f,\n",
    "      pressures_f\n",
    "    ))\n",
    "    return payloads, model\n",
    "\n",
    "  else:\n",
    "    return clusterpayloadfeatures(payloads, flags, train_data_model)\n",
    "\n",
    "def imputebykeepinglastvalue(features, not_nans):\n",
    "  first_not_nan = not_nans[0]\n",
    "  features[:first_not_nan] = features[first_not_nan]\n",
    "\n",
    "  for begin, end in pairwise(not_nans):\n",
    "    features[begin:end] = features[begin]\n",
    "\n",
    "  '''\n",
    "  Case: To keep the value until the end of dataset\n",
    "  '''\n",
    "  last = len(features)\n",
    "  last_not_nan = not_nans[-1]\n",
    "  if last != last_not_nan:\n",
    "    features[last_not_nan+1:] = features[last_not_nan]\n",
    "\n",
    "def grouppayloads(payloads):\n",
    "  '''\n",
    "   Last two columns uniquely identify a category of payload\n",
    "    both columns are NaNs -> 0\n",
    "    first is NaN and second is not NaN -> 1\n",
    "    first is not NaN and second is NaN -> 2\n",
    "  Returns indicies\n",
    "  '''\n",
    "  ids = payloads[:,-2:]\n",
    "  ids = np.packbits(~np.isnan(ids), axis=1) // 64\n",
    "  return ids.reshape(-1)\n",
    "\n",
    "def preprocessfunctioncodes(functions, flags, train_data_model):\n",
    "\n",
    "  if flags.encode_function:    \n",
    "    '''\n",
    "    Encode function codes is the same for training and non-training data\n",
    "    '''\n",
    "    print('Encoding function...')\n",
    "    encoder = LabelEncoder()\n",
    "    functions = encoder.fit_transform(functions.reshape(-1))\n",
    "    return np_utils.to_categorical(functions), None\n",
    "\n",
    "  if not train_data_model:\n",
    "\n",
    "    if flags.cluster_function_kmeans != None:\n",
    "      k = flags.cluster_function_kmeans\n",
    "      predicted, model = clusterkmeans(functions, 'function', elbow=flags.elbow, k=k)\n",
    "      return np_utils.to_categorical(predicted), model\n",
    "\n",
    "    elif flags.cluster_function_gmm:\n",
    "      k = flags.cluster_function_gmm\n",
    "      predicted, model = clustergmm(functions, 'function', elbow=flags.elbow, k=k)\n",
    "      return np_utils.to_categorical(predicted), model\n",
    "\n",
    "  else:\n",
    "    return np_utils.to_categorical(train_data_model.predict(functions)), None\n",
    "\n",
    "def preprocessdata(Xs, flags, train_data_models = None):\n",
    "  \n",
    "  addresses = Xs[:, 0].reshape((-1, 1))\n",
    "  responses = Xs[:, 2].reshape((-1, 1))\n",
    "\n",
    "  functions = Xs[:, 1].reshape((-1, 1))\n",
    "  payloads = Xs[:, 3:14]\n",
    "  \n",
    "  models = None\n",
    "\n",
    "  if not train_data_models:\n",
    "    functions, f_model = preprocessfunctioncodes(functions, flags, None)\n",
    "    payloads, p_model  = preprocesspayloadfeatures(payloads, flags, None)\n",
    "    models = {'function': f_model, 'payload': p_model}\n",
    "\n",
    "  else:\n",
    "    f_model = train_data_models['function']\n",
    "    p_model = train_data_models['payload']\n",
    "\n",
    "    functions, _ = preprocessfunctioncodes(functions, flags, f_model)\n",
    "    payloads, _  = preprocesspayloadfeatures(payloads, flags, p_model)  \n",
    "\n",
    "  stacked = np.column_stack((\n",
    "    addresses, \n",
    "    functions, \n",
    "    responses,\n",
    "    payloads\n",
    "  ))\n",
    "\n",
    "  if Xs.shape[1] >= 14:\n",
    "    print('Additional features were found!')\n",
    "    remaining = Xs[:, 14:]\n",
    "\n",
    "    if not train_data_models:\n",
    "      remaining, r_model = normalize(remaining, flags.normalize, None)\n",
    "\n",
    "      models['remaining'] = r_model\n",
    "    else:\n",
    "      r_model = train_data_models['remaining']\n",
    "      remaining, _ = normalize(remaining, flags.normalize, r_model)\n",
    "\n",
    "    stacked = np.column_stack((stacked, remaining))\n",
    "\n",
    "  return stacked, models\n",
    "\n",
    "def cluster(model_class, score_func, Xs, feature_name, k, elbow, max_k):\n",
    "  \n",
    "  if elbow:\n",
    "    models = [model_class(i).fit(Xs) for i in range(1, max_k+1)]\n",
    "    scores = [score_func(model, Xs) for model in models]\n",
    "    k = promptelbowmethod(scores, feature_name)\n",
    "\n",
    "  m = model_class(k).fit(Xs)\n",
    "  return m.predict(Xs), m\n",
    "\n",
    "def clustergmm(Xs, feature_name, k=None, elbow=True):\n",
    "  \n",
    "  print('Clustering {} with GMM'.format(feature_name))\n",
    "  GMM = lambda k: GaussianMixture(n_components=k, init_params='random')\n",
    "  score_func = lambda gmm, Xs: gmm.aic(Xs)\n",
    "  return cluster(GMM, score_func, Xs, feature_name, k, elbow, max_k=20)\n",
    "\n",
    "def clusterkmeans(Xs, feature_name, k=None, elbow=True):\n",
    "  \n",
    "  print('Clustering {} with Kmeans'.format(feature_name))\n",
    "  KM = lambda k: KMeans(n_clusters=k)\n",
    "  score_func = lambda km, Xs: km.score(Xs)\n",
    "  return cluster(KM, score_func, Xs, feature_name, k, elbow, max_k=10)\n",
    "\n",
    "def promptelbowmethod(scores, feature_name):\n",
    "  print(scores)\n",
    "  xs = range(1, len(scores)+1)\n",
    "  ys = np.array(scores)\n",
    "  is_log = ''\n",
    "  if np.all(ys > 0) or np.all(ys < 0):\n",
    "    ys = np.log(np.abs(ys))\n",
    "    is_log = 'log'\n",
    "  print(ys)\n",
    "  plt.plot(xs, ys)\n",
    "  plt.xticks(xs)\n",
    "  plt.xlabel('Number of clusters')\n",
    "  plt.ylabel('{} Score'.format(is_log))\n",
    "  plt.title('Elbow method for {}'.format(feature_name))\n",
    "  plt.show()\n",
    "  print('Please enter number of clusters for {}:'.format(feature_name))\n",
    "  k = int(input('-->'))\n",
    "  print('\\nselected k: {}'.format(k))\n",
    "  return k\n",
    "\n",
    "#-----------------------------------#\n",
    "# Run Data Pre-processing Main Code #\n",
    "#-----------------------------------#\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser(description='Preprocess SCADA dataset.')\n",
    "  parser.add_argument('-d', '--dataset', type=str, help='Dataset filename')\n",
    "  parser.add_argument('--label', type=str, choices=['binary', 'category', 'dcategory'],\n",
    "    default='binary', help='Type of labels to output in the preprocessed dataset')\n",
    "  parser.add_argument('-t', '--time-series', action='store_true', \n",
    "    help='Keep temporal structure of the data')\n",
    "  parser.add_argument('-n', '--normalize', type=str, choices=['mean', 'minmax'],\n",
    "    default='mean', help='Type of normalization to preform')\n",
    "  parser.add_argument('--discard-crc', action='store_true', \n",
    "    help='Remove CRC feature from the dataset')\n",
    "  parser.add_argument('--discard-length', action='store_true', \n",
    "    help='Remove Length feature from the dataset')\n",
    "\n",
    "  function_encoding_group = parser.add_mutually_exclusive_group(required=True)\n",
    "  function_encoding_group.add_argument('--encode-function', action='store_true',\n",
    "    help='Encode function codes with One-Hot encoding')\n",
    "  function_encoding_group.add_argument('--cluster-function-kmeans', type=int,\n",
    "    help='Cluster function codes with kmeans')\n",
    "  function_encoding_group.add_argument('--cluster-function-gmm', type=int,\n",
    "    help='Cluster function codes with GMM')\n",
    "\n",
    "  payload_features_impution_group = parser.add_mutually_exclusive_group(required=True)\n",
    "  payload_features_impution_group.add_argument('--payload-keep-value-imputed', action='store_true',\n",
    "    help='Impute payload features by keeping the oldest value')\n",
    "  payload_features_impution_group.add_argument('--payload-indicator-imputed', action='store_true',\n",
    "    help='Impute payload features with 0\\'s and add indicators')    \n",
    "  payload_features_impution_group.add_argument('--payload-kmeans-imputed', type=int, nargs=2,\n",
    "    help='Impute payload features by clustering with kmeans, first for pressure second for remaining')\n",
    "  payload_features_impution_group.add_argument('--payload-gmm-imputed', type=int, nargs=2,\n",
    "    help='Impute payload features by clustering with GMM, first for pressure second for remaining')    \n",
    "\n",
    "  parser.add_argument('--elbow', action='store_true', \n",
    "    help='When using clustering, use elbow method to get best number of clusters')\n",
    "\n",
    "  parser.add_argument('-o', '--output', type=str, help='Output .npy files')\n",
    "  parser.add_argument('--split', type=float, nargs=3, help='train/val/test split ratio')\n",
    "\n",
    "  flags = parser.parse_args('-d ./dataset/IanArffDataset.arff --split 0.6 0.2 0.2 --encode-function --label binary --time-series -n mean --payload-indicator-imputed  -o processeddataset'.split())\n",
    "\n",
    "  dataset, meta = arff.loadarff(flags.dataset)\n",
    "\n",
    "  label_types = {\n",
    "    'binary'    : 'binary result',\n",
    "    'category'  : 'categorized result',\n",
    "    'dcategory' : 'specific result'\n",
    "  }\n",
    "\n",
    "  print('____________________________________________')\n",
    "  print('START Data Pre-processing')\n",
    "  print('____________________________________________')\n",
    "  print('Encoding labels...')\n",
    "  label_name = label_types[flags.label]\n",
    "  labels = dataset[label_name].astype(np.float)\n",
    "  labels = labels.reshape((-1, 1))\n",
    "  \n",
    "  \n",
    "  print('Encoding addressess...')\n",
    "  #pre-process address before splitting, simple change of values and reshape into column vec\n",
    "  addresses = preprocessing.label_binarize(dataset['address'], classes=[4])\n",
    "\n",
    "  print('Encoding functions...')\n",
    "  functions = dataset['function'].astype(np.float).reshape((-1, 1))\n",
    "\n",
    "  print('Encoding command responses...')\n",
    "  #pre-process address before splitting, parse string and reshape into column vec\n",
    "  responses = dataset['command response'].astype(np.float).reshape((-1, 1))\n",
    "\n",
    "  print('Extracting payload features')\n",
    "  payload_feature_names = meta.names()[3:14]\n",
    "    \n",
    "  payload_features = repack_fields(dataset[payload_feature_names])\n",
    "  \n",
    "  payload_features = payload_features \\\n",
    "    .view(np.float64) \\\n",
    "    .reshape(payload_features.shape + (-1,))\n",
    "\n",
    "  Xs = np.column_stack((\n",
    "    addresses, \n",
    "    functions, \n",
    "    responses,\n",
    "    payload_features\n",
    "  ))\n",
    "\n",
    "  if not flags.discard_length:\n",
    "    print('\\tAdding length...')\n",
    "    lengths = dataset['length'].astype(np.float).reshape((-1, 1))\n",
    "    Xs = np.column_stack((Xs, lengths))\n",
    "\n",
    "  if not flags.discard_crc:\n",
    "    print('\\tAdding crc rate...')\n",
    "    crcs = dataset['crc rate'].astype(np.float).reshape((-1, 1))\n",
    "    Xs = np.column_stack((Xs, crcs))\n",
    "\n",
    "  if flags.time_series:\n",
    "    print('\\tAdding timestamp differences...')\n",
    "    Xs = np.column_stack((Xs, dataset['time']))\n",
    "    \n",
    "\n",
    "  if flags.payload_keep_value_imputed:\n",
    "\n",
    "    Xs = preprocessdatafulldataset(Xs,flags)\n",
    "    Xs, labels = sklearn.utils.shuffle(Xs, labels)\n",
    "\n",
    "    print('\\n\\tSplitting dataset...')\n",
    "    Xs, Ys = splitdataset(Xs, labels, *flags.split)\n",
    "    Xs_train, Xs_val, Xs_test = Xs\n",
    "    Ys_train, Ys_val, Ys_test = Ys\n",
    "\n",
    "    Xs_train_val = np.concatenate((Xs_train,Xs_val))\n",
    "    Ys_train_val = np.concatenate((Ys_train,Ys_val))\n",
    "\n",
    "    Xs_train_test = Xs_test\n",
    "    Ys_train_test = Ys_test\n",
    "\n",
    "    Xs_train, train_models = preprocessdata(Xs_train, flags)\n",
    "    Xs_train_val, train_models_val = preprocessdata(Xs_train_val, flags)\n",
    "    Xs_val,  _ = preprocessdata(Xs_val,  flags, train_data_models=train_models)\n",
    "    Xs_test, _ = preprocessdata(Xs_test, flags, train_data_models=train_models)\n",
    "    Xs_train_test, _ = preprocessdata(Xs_train_test, flags, train_data_models=train_models_val)\n",
    "\n",
    "  else:\n",
    "    print('\\n\\tSplitting dataset...')\n",
    "    Xs, labels = sklearn.utils.shuffle(Xs, labels)\n",
    "    Xs, Ys = splitdataset(Xs, labels, *flags.split)\n",
    "    Xs_train, Xs_val, Xs_test = Xs\n",
    "    Ys_train, Ys_val, Ys_test = Ys\n",
    "\n",
    "    Xs_train_val = np.concatenate((Xs_train,Xs_val))\n",
    "    Ys_train_val = np.concatenate((Ys_train,Ys_val))\n",
    "    Xs_train_test = Xs_test\n",
    "    Ys_train_test = Ys_test\n",
    "\n",
    "    Xs_train, train_models = preprocessdata(Xs_train, flags)\n",
    "    Xs_val,  _ = preprocessdata(Xs_val,  flags, train_data_models=train_models)\n",
    "    Xs_test, _ = preprocessdata(Xs_test, flags, train_data_models=train_models)\n",
    "\n",
    "    Xs_train_val, train_models_val = preprocessdata(Xs_train_val, flags)\n",
    "    Xs_train_test, _ = preprocessdata(Xs_train_test, flags, train_data_models=train_models_val)\n",
    "\n",
    "    print(Xs_train_val.shape, Ys_train_val.shape)\n",
    "    print(Xs_train_test.shape, Ys_train_test.shape)\n",
    "\n",
    "\n",
    "  if not flags.time_series:\n",
    "    Xs_train, Ys_train = sklearn.utils.shuffle(Xs_train, Ys_train, random_state=0)\n",
    " \n",
    "  output_dir = flags.output\n",
    "\n",
    "  if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "  np.save(os.path.join(output_dir, 'Xs_train'), Xs_train)\n",
    "  np.save(os.path.join(output_dir, 'Xs_val'),   Xs_val)\n",
    "  np.save(os.path.join(output_dir, 'Xs_test'),  Xs_test)\n",
    "\n",
    "  np.save(os.path.join(output_dir, 'Ys_train'), Ys_train)\n",
    "  np.save(os.path.join(output_dir, 'Ys_val'),   Ys_val)\n",
    "  np.save(os.path.join(output_dir, 'Ys_test'),  Ys_test)\n",
    "\n",
    "  np.save(os.path.join(output_dir, 'Xs_train_val'), Xs_train_val)\n",
    "  np.save(os.path.join(output_dir, 'Xs_train_test'), Xs_train_test)\n",
    "  np.save(os.path.join(output_dir, 'Ys_train_val'), Ys_train_val)\n",
    "  np.save(os.path.join(output_dir, 'Ys_train_test'), Ys_train_test)\n",
    "    \n",
    "  print('____________________________________________')\n",
    "  print('END Data Pre-processing')\n",
    "  print('____________________________________________')\n",
    "    \n",
    "    \n",
    "#-----------------------#  \n",
    "# PREDICTION MODEL CODE #\n",
    "#-----------------------#\n",
    "\n",
    "class ReportMetric(tensorflow.keras.callbacks.Callback):\n",
    "\n",
    "  def __init__(self, valid_data, label_dim, bs, file):\n",
    "    self.valid_data = valid_data\n",
    "    self.label_dim = label_dim\n",
    "    self.bs = bs\n",
    "    self.file = file\n",
    "\n",
    "  def on_epoch_end(self, batch, logs={}):\n",
    "    if self.label_dim != 1:\n",
    "      Y_pred = np.argmax(model.predict(self.valid_data[0], batch_size=self.bs, verbose=1), axis=-1).reshape(-1)\n",
    "      Y_true = np.argmax(self.valid_data[1], axis=2).reshape(-1)\n",
    "    else:\n",
    "      Y_pred = (model.predict(self.valid_data[0], batch_size=self.bs, verbose=1) > 0.5).astype(\"int32\").reshape(-1)\n",
    "      Y_true = self.valid_data[1].reshape(-1)\n",
    "\n",
    "    report = classification_report(Y_true, Y_pred,digits=4)\n",
    "    conf_matrix  = confusion_matrix(Y_true, Y_pred)\n",
    "    self.file.write('\\n' + report + '\\n')\n",
    "    print(report)\n",
    "    print(conf_matrix)\n",
    "\n",
    "def make_sequences(Xs, Ys, seqlen, step = 1):\n",
    "  Xseq, Yseq = [], []\n",
    "  for i in range(0, Xs.shape[0] - seqlen + 1, step):\n",
    "    Xseq.append(Xs[i: i+seqlen])\n",
    "    Yseq.append(Ys[i: i+seqlen])\n",
    "  return np.array(Xseq), np.array(Yseq)\n",
    "\n",
    "def lstm_model(input_dim, output_dim, seq_len, hidden=128, dropout=0.0, lr=0.1):\n",
    "  model = Sequential()\n",
    "  layers = {'input': input_dim, 'hidden': hidden, 'output': output_dim}\n",
    "  \n",
    "  model.add(Bidirectional(LSTM(layers['input'], return_sequences=True),\n",
    "    merge_mode='concat', input_shape=(seq_len, layers['input'])))\n",
    "  model.add(Dropout(dropout))\n",
    "\n",
    "  activation = 'softmax' if output_dim > 1 else 'sigmoid'\n",
    "  loss = 'categorical_crossentropy' if output_dim > 1 else 'binary_crossentropy'\n",
    "\n",
    "  model.add(TimeDistributed(Dense(layers['output'], activation=activation)))\n",
    "\n",
    "  model.compile(loss=loss, optimizer=Adam(lr=lr), metrics=['acc'])\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser(description='Run LSTM')\n",
    "  parser.add_argument('-d', '--dataset', required=True, type=str, help='Data directory')\n",
    "  parser.add_argument('-i', '--iters', default=20, type=int, help='Number of random samples')\n",
    "  \n",
    "  flags = parser.parse_args('-d ./processeddataset/ --i 2'.split())\n",
    "\n",
    "  dataset_dir = flags.dataset\n",
    "  dataset_filenames = [\n",
    "    'Xs_train.npy', 'Xs_val.npy', 'Xs_test.npy', 'Xs_train_val.npy','Xs_train_test.npy',\n",
    "    'Ys_train.npy', 'Ys_val.npy', 'Ys_test.npy', 'Ys_train_val.npy', 'Ys_train_test.npy'\n",
    "  ]\n",
    "  dataset_filenames = map(lambda x: os.path.join(dataset_dir, x), dataset_filenames)\n",
    "  X_train, X_valid, X_test, Xs_train_val, Xs_train_test, Y_train, Y_valid, Y_test, Ys_train_val, Ys_train_test  = map(np.load, dataset_filenames)\n",
    "  \n",
    "  # For evaluation on test set - uncomment below code #\n",
    "  X_train = Xs_train_val\n",
    "  Y_train = Ys_train_val\n",
    "  X_test = Xs_train_test\n",
    "  Y_test = Ys_train_test\n",
    "\n",
    "  print('____________________________________________')\n",
    "  print('START Building Model')\n",
    "  print('____________________________________________')\n",
    "    \n",
    "  print(X_train.shape, Y_train.shape)\n",
    "  print(X_valid.shape, Y_valid.shape)\n",
    "  print(X_test.shape, Y_test.shape)\n",
    "\n",
    "  n_label, label_dim = Y_train.shape\n",
    "  sums = np.sum(Y_train, axis=0)\n",
    "  \n",
    "  print('Label dimension: {}'.format(label_dim))\n",
    "  if not label_dim in [1, 8, 36]:\n",
    "    raise Exception('!Unknown label dimension: {}'.format(label_dim))\n",
    "\n",
    "  input_dim = X_train.shape[1]\n",
    "  print('Input dimension: {}'.format(input_dim))\n",
    "\n",
    "  iters = flags.iters\n",
    "  print('Number of iterations: {}'.format(iters))\n",
    "  learning_rate = lambda: 10 ** np.random.uniform(-3, -1)\n",
    "  sequence_length = lambda: np.random.choice([4])\n",
    "  hidden_layer_size = lambda: int(2 ** np.random.uniform(6, 8))\n",
    "  batch_size = lambda: int(2 ** np.random.uniform(6, 8))\n",
    "  dropout = lambda: np.random.uniform(0.0, 0.2)\n",
    "  step = lambda: 4\n",
    "  n_epochs = lambda: 10\n",
    "  \n",
    "  ranges = [\n",
    "    learning_rate,\n",
    "    batch_size,\n",
    "    n_epochs,\n",
    "    sequence_length,\n",
    "    dropout,\n",
    "    hidden_layer_size,\n",
    "    step\n",
    "  ]\n",
    "\n",
    "  with open(os.path.join(dataset_dir, 'aggregate.txt'), 'w') as f:\n",
    "    f.write('Learning Rate,Batch size,Number of epochs,Sequence length,Dropout rate,Hidden layer size\\n')\n",
    "\n",
    "  for iterations in range(iters):\n",
    "    #p() is calling all functions in the list\n",
    "    hyperparams = [p() for p in ranges]\n",
    "    (lr, bs, ne, sl, dr, hls, step) = hyperparams\n",
    "    hparams = ','.join(map(str, hyperparams))\n",
    "\n",
    "    print('Hyperparameters setting:')\n",
    "    print(hparams)\n",
    "\n",
    "    output_dir = os.path.join(dataset_dir, 'results')\n",
    "    if not os.path.exists(output_dir):\n",
    "      os.makedirs(output_dir)\n",
    "\n",
    "    model = lstm_model(input_dim, label_dim, sl, \\\n",
    "      hidden=hls, dropout=dr, lr=lr)\n",
    "\n",
    "    print('Preparing sequences - Length = {}'.format(sl))\n",
    "\n",
    "    X_train_seq, Y_train_seq = make_sequences(X_train, Y_train, sl, step)\n",
    "    X_valid_seq, Y_valid_seq = make_sequences(X_valid, Y_valid, sl, step)\n",
    "    X_test_seq,  Y_test_seq  = make_sequences(X_test,  Y_test,  sl, step)\n",
    "    \n",
    "    print('____________________________________________')\n",
    "    print('MODEL EVALUATION')\n",
    "    print('____________________________________________')\n",
    "\n",
    "    print(X_train_seq.shape,Y_train_seq.shape)\n",
    "    print(X_valid_seq.shape,Y_valid_seq.shape)\n",
    "    print(X_test_seq.shape, Y_test_seq.shape)\n",
    "\n",
    "    output_file = os.path.join(output_dir, '-'.join(map(str, hyperparams)) + '.txt')\n",
    "    with open(output_file, 'w') as f:\n",
    "\n",
    "      log_dir = os.path.join(output_dir, '_'.join(map(str, hyperparams)))\n",
    "      if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "      \n",
    "      chpt_filepath = os.path.join(log_dir, 'weights.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "\n",
    "      smcb = tensorflow.keras.callbacks.ModelCheckpoint(chpt_filepath)\n",
    "      tbcb = tensorflow.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True)\n",
    "      valid_data = (X_valid_seq, Y_valid_seq)\n",
    "      rmcb = ReportMetric(valid_data, label_dim, bs, f) \n",
    "\n",
    "      reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=3, min_lr=0.001)\n",
    "\n",
    "      # Training Set:\n",
    "    \n",
    "#       h = model.fit(X_train_seq, Y_train_seq, \\\n",
    "#         validation_data=(X_valid_seq, Y_valid_seq), batch_size=bs, epochs=ne, \\\n",
    "#         callbacks=[tbcb, smcb, rmcb, reduce_lr]\n",
    "#       )\n",
    "\n",
    "      # For Test set - uncomment below code\n",
    "\n",
    "      h = model.fit(X_train_seq, Y_train_seq, \\\n",
    "        validation_data=(X_test_seq, Y_test_seq), batch_size=bs, epochs=ne, \\\n",
    "        callbacks=[tbcb, smcb, rmcb, reduce_lr]\n",
    "      )\n",
    "    \n",
    "      for k,v in h.history.items():\n",
    "        f.write(k + ':' + ','.join(map(str, v)) + '\\n')\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
