{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________\n",
      "START Data Pre-processing\n",
      "____________________________________________\n",
      "Encoding labels...\n",
      "Encoding addressess...\n",
      "Encoding functions...\n",
      "Encoding command responses...\n",
      "Extracting payload features\n",
      "\tAdding length...\n",
      "\tAdding crc rate...\n",
      "\tAdding timestamp differences...\n",
      "\n",
      "\tSplitting dataset...\n",
      "Encoding function...\n",
      "Imputing payload features using indicators\n",
      "Additional features were found!\n",
      "Encoding function...\n",
      "Imputing payload features using indicators\n",
      "Additional features were found!\n",
      "Encoding function...\n",
      "Imputing payload features using indicators\n",
      "Additional features were found!\n",
      "Encoding function...\n",
      "Imputing payload features using indicators\n",
      "Additional features were found!\n",
      "Encoding function...\n",
      "Imputing payload features using indicators\n",
      "Additional features were found!\n",
      "(219701, 57) (219701, 1)\n",
      "(54927, 57) (54927, 1)\n",
      "____________________________________________\n",
      "END Data Pre-processing\n",
      "____________________________________________\n",
      "____________________________________________\n",
      "START Building Model\n",
      "____________________________________________\n",
      "(219701, 57) (219701, 1)\n",
      "(54925, 57) (54925, 1)\n",
      "(54927, 57) (54927, 1)\n",
      "Label dimension: 1\n",
      "Input dimension: 57\n",
      "Number of iterations: 2\n",
      "Hyperparameters setting:\n",
      "0.012998731043485737,154,10,4,0.10857915513542833,220,4\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, 4, 114)            52440     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 114)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 4, 1)              115       \n",
      "=================================================================\n",
      "Total params: 52,555\n",
      "Trainable params: 52,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Preparing sequences - Length = 4\n",
      "____________________________________________\n",
      "MODEL EVALUATION\n",
      "____________________________________________\n",
      "(54925, 4, 57) (54925, 4, 1)\n",
      "(13731, 4, 57) (13731, 4, 1)\n",
      "(13731, 4, 57) (13731, 4, 1)\n",
      "Epoch 1/10\n",
      "  3/357 [..............................] - ETA: 10:45 - loss: 0.5983 - acc: 0.7361WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0088s vs `on_train_batch_end` time: 0.6037s). Check your callbacks.\n",
      "357/357 [==============================] - 11s 20ms/step - loss: 0.3660 - acc: 0.8543 - val_loss: 0.2842 - val_acc: 0.8909\n",
      "90/90 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8777    0.9971    0.9336     42859\n",
      "         1.0     0.9803    0.5065    0.6679     12065\n",
      "\n",
      "    accuracy                         0.8894     54924\n",
      "   macro avg     0.9290    0.7518    0.8008     54924\n",
      "weighted avg     0.9002    0.8894    0.8753     54924\n",
      "\n",
      "[[42736   123]\n",
      " [ 5954  6111]]\n",
      "Epoch 2/10\n",
      "357/357 [==============================] - 2s 6ms/step - loss: 0.2814 - acc: 0.8919 - val_loss: 0.2782 - val_acc: 0.8931\n",
      "90/90 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8790    0.9988    0.9350     42859\n",
      "         1.0     0.9915    0.5114    0.6748     12065\n",
      "\n",
      "    accuracy                         0.8917     54924\n",
      "   macro avg     0.9352    0.7551    0.8049     54924\n",
      "weighted avg     0.9037    0.8917    0.8779     54924\n",
      "\n",
      "[[42806    53]\n",
      " [ 5895  6170]]\n",
      "Epoch 3/10\n",
      "357/357 [==============================] - 2s 6ms/step - loss: 0.2751 - acc: 0.8941 - val_loss: 0.2754 - val_acc: 0.8939\n",
      "90/90 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8796    0.9990    0.9355     42859\n",
      "         1.0     0.9928    0.5145    0.6777     12065\n",
      "\n",
      "    accuracy                         0.8925     54924\n",
      "   macro avg     0.9362    0.7567    0.8066     54924\n",
      "weighted avg     0.9045    0.8925    0.8789     54924\n",
      "\n",
      "[[42814    45]\n",
      " [ 5858  6207]]\n",
      "Epoch 4/10\n",
      "357/357 [==============================] - 2s 6ms/step - loss: 0.2683 - acc: 0.8956 - val_loss: 0.2713 - val_acc: 0.8941\n",
      "90/90 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8809    0.9974    0.9355     42859\n",
      "         1.0     0.9823    0.5208    0.6807     12065\n",
      "\n",
      "    accuracy                         0.8927     54924\n",
      "   macro avg     0.9316    0.7591    0.8081     54924\n",
      "weighted avg     0.9031    0.8927    0.8795     54924\n",
      "\n",
      "[[42746   113]\n",
      " [ 5782  6283]]\n",
      "Epoch 5/10\n",
      "357/357 [==============================] - 2s 6ms/step - loss: 0.2671 - acc: 0.8945 - val_loss: 0.2680 - val_acc: 0.8947\n",
      "90/90 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8815    0.9980    0.9362     42859\n",
      "         1.0     0.9866    0.5236    0.6841     12065\n",
      "\n",
      "    accuracy                         0.8938     54924\n",
      "   macro avg     0.9341    0.7608    0.8101     54924\n",
      "weighted avg     0.9046    0.8938    0.8808     54924\n",
      "\n",
      "[[42773    86]\n",
      " [ 5748  6317]]\n",
      "Epoch 6/10\n",
      "357/357 [==============================] - 2s 6ms/step - loss: 0.2603 - acc: 0.8971 - val_loss: 0.2689 - val_acc: 0.8946\n",
      "90/90 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8810    0.9995    0.9365     42859\n",
      "         1.0     0.9965    0.5204    0.6838     12065\n",
      "\n",
      "    accuracy                         0.8943     54924\n",
      "   macro avg     0.9388    0.7600    0.8101     54924\n",
      "weighted avg     0.9064    0.8943    0.8810     54924\n",
      "\n",
      "[[42837    22]\n",
      " [ 5786  6279]]\n",
      "Epoch 7/10\n",
      "357/357 [==============================] - 2s 6ms/step - loss: 0.2579 - acc: 0.8973 - val_loss: 0.2647 - val_acc: 0.8953\n",
      "90/90 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8829    0.9988    0.9373     42859\n",
      "         1.0     0.9922    0.5292    0.6903     12065\n",
      "\n",
      "    accuracy                         0.8957     54924\n",
      "   macro avg     0.9375    0.7640    0.8138     54924\n",
      "weighted avg     0.9069    0.8957    0.8830     54924\n",
      "\n",
      "[[42809    50]\n",
      " [ 5680  6385]]\n",
      "Epoch 8/10\n",
      "357/357 [==============================] - 2s 6ms/step - loss: 0.2574 - acc: 0.8964 - val_loss: 0.2661 - val_acc: 0.8952\n",
      "90/90 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8841    0.9985    0.9378     42859\n",
      "         1.0     0.9899    0.5352    0.6947     12065\n",
      "\n",
      "    accuracy                         0.8967     54924\n",
      "   macro avg     0.9370    0.7668    0.8163     54924\n",
      "weighted avg     0.9074    0.8967    0.8844     54924\n",
      "\n",
      "[[42793    66]\n",
      " [ 5608  6457]]\n",
      "Epoch 9/10\n",
      "357/357 [==============================] - 2s 6ms/step - loss: 0.2507 - acc: 0.8998 - val_loss: 0.2638 - val_acc: 0.8960\n",
      "90/90 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8847    0.9990    0.9384     42859\n",
      "         1.0     0.9936    0.5375    0.6976     12065\n",
      "\n",
      "    accuracy                         0.8976     54924\n",
      "   macro avg     0.9391    0.7683    0.8180     54924\n",
      "weighted avg     0.9086    0.8976    0.8855     54924\n",
      "\n",
      "[[42817    42]\n",
      " [ 5580  6485]]\n",
      "Epoch 10/10\n",
      "357/357 [==============================] - 2s 6ms/step - loss: 0.2484 - acc: 0.8997 - val_loss: 0.2638 - val_acc: 0.8955\n",
      "90/90 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8864    0.9983    0.9390     42859\n",
      "         1.0     0.9887    0.5457    0.7033     12065\n",
      "\n",
      "    accuracy                         0.8988     54924\n",
      "   macro avg     0.9376    0.7720    0.8211     54924\n",
      "weighted avg     0.9089    0.8988    0.8872     54924\n",
      "\n",
      "[[42784    75]\n",
      " [ 5481  6584]]\n",
      "Hyperparameters setting:\n",
      "0.013830114265111531,64,10,4,0.11476436737668423,124,4\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_3 (Bidirection (None, 4, 114)            52440     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 114)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 4, 1)              115       \n",
      "=================================================================\n",
      "Total params: 52,555\n",
      "Trainable params: 52,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Preparing sequences - Length = 4\n",
      "____________________________________________\n",
      "MODEL EVALUATION\n",
      "____________________________________________\n",
      "(54925, 4, 57) (54925, 4, 1)\n",
      "(13731, 4, 57) (13731, 4, 1)\n",
      "(13731, 4, 57) (13731, 4, 1)\n",
      "Epoch 1/10\n",
      "  3/859 [..............................] - ETA: 25:46 - loss: 0.7034 - acc: 0.4145WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0049s vs `on_train_batch_end` time: 0.6002s). Check your callbacks.\n",
      "859/859 [==============================] - 13s 10ms/step - loss: 0.3419 - acc: 0.8621 - val_loss: 0.2834 - val_acc: 0.8915\n",
      "215/215 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8783    0.9973    0.9340     42859\n",
      "         1.0     0.9815    0.5092    0.6706     12065\n",
      "\n",
      "    accuracy                         0.8901     54924\n",
      "   macro avg     0.9299    0.7533    0.8023     54924\n",
      "weighted avg     0.9010    0.8901    0.8762     54924\n",
      "\n",
      "[[42743   116]\n",
      " [ 5921  6144]]\n",
      "Epoch 2/10\n",
      "859/859 [==============================] - 4s 5ms/step - loss: 0.2770 - acc: 0.8941 - val_loss: 0.2739 - val_acc: 0.8940\n",
      "215/215 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8793    0.9988    0.9352     42859\n",
      "         1.0     0.9917    0.5129    0.6761     12065\n",
      "\n",
      "    accuracy                         0.8921     54924\n",
      "   macro avg     0.9355    0.7558    0.8057     54924\n",
      "weighted avg     0.9040    0.8921    0.8783     54924\n",
      "\n",
      "[[42807    52]\n",
      " [ 5877  6188]]\n",
      "Epoch 3/10\n",
      "859/859 [==============================] - 4s 5ms/step - loss: 0.2716 - acc: 0.8940 - val_loss: 0.2713 - val_acc: 0.8947\n",
      "215/215 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8806    0.9983    0.9358     42859\n",
      "         1.0     0.9888    0.5191    0.6808     12065\n",
      "\n",
      "    accuracy                         0.8931     54924\n",
      "   macro avg     0.9347    0.7587    0.8083     54924\n",
      "weighted avg     0.9044    0.8931    0.8798     54924\n",
      "\n",
      "[[42788    71]\n",
      " [ 5802  6263]]\n",
      "Epoch 4/10\n",
      "859/859 [==============================] - 4s 5ms/step - loss: 0.2655 - acc: 0.8961 - val_loss: 0.2674 - val_acc: 0.8947\n",
      "215/215 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8805    0.9994    0.9362     42859\n",
      "         1.0     0.9959    0.5184    0.6818     12065\n",
      "\n",
      "    accuracy                         0.8937     54924\n",
      "   macro avg     0.9382    0.7589    0.8090     54924\n",
      "weighted avg     0.9059    0.8937    0.8803     54924\n",
      "\n",
      "[[42833    26]\n",
      " [ 5811  6254]]\n",
      "Epoch 5/10\n",
      "859/859 [==============================] - 4s 5ms/step - loss: 0.2621 - acc: 0.8962 - val_loss: 0.2619 - val_acc: 0.8958\n",
      "215/215 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8820    0.9992    0.9369     42859\n",
      "         1.0     0.9945    0.5252    0.6874     12065\n",
      "\n",
      "    accuracy                         0.8951     54924\n",
      "   macro avg     0.9383    0.7622    0.8121     54924\n",
      "weighted avg     0.9067    0.8951    0.8821     54924\n",
      "\n",
      "[[42824    35]\n",
      " [ 5729  6336]]\n",
      "Epoch 6/10\n",
      "859/859 [==============================] - 4s 5ms/step - loss: 0.2584 - acc: 0.8967 - val_loss: 0.2646 - val_acc: 0.8957\n",
      "215/215 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8839    0.9976    0.9373     42859\n",
      "         1.0     0.9846    0.5344    0.6928     12065\n",
      "\n",
      "    accuracy                         0.8959     54924\n",
      "   macro avg     0.9342    0.7660    0.8151     54924\n",
      "weighted avg     0.9060    0.8959    0.8836     54924\n",
      "\n",
      "[[42758   101]\n",
      " [ 5617  6448]]\n",
      "Epoch 7/10\n",
      "859/859 [==============================] - 4s 5ms/step - loss: 0.2573 - acc: 0.8971 - val_loss: 0.2610 - val_acc: 0.8966\n",
      "215/215 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8840    0.9986    0.9378     42859\n",
      "         1.0     0.9905    0.5344    0.6942     12065\n",
      "\n",
      "    accuracy                         0.8966     54924\n",
      "   macro avg     0.9372    0.7665    0.8160     54924\n",
      "weighted avg     0.9074    0.8966    0.8843     54924\n",
      "\n",
      "[[42797    62]\n",
      " [ 5618  6447]]\n",
      "Epoch 8/10\n",
      "859/859 [==============================] - 4s 5ms/step - loss: 0.2515 - acc: 0.8992 - val_loss: 0.2615 - val_acc: 0.8966\n",
      "215/215 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8846    0.9988    0.9382     42859\n",
      "         1.0     0.9922    0.5372    0.6970     12065\n",
      "\n",
      "    accuracy                         0.8974     54924\n",
      "   macro avg     0.9384    0.7680    0.8176     54924\n",
      "weighted avg     0.9082    0.8974    0.8853     54924\n",
      "\n",
      "[[42808    51]\n",
      " [ 5584  6481]]\n",
      "Epoch 9/10\n",
      "859/859 [==============================] - 5s 5ms/step - loss: 0.2518 - acc: 0.8986 - val_loss: 0.2576 - val_acc: 0.8975\n",
      "215/215 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8867    0.9986    0.9393     42859\n",
      "         1.0     0.9908    0.5467    0.7046     12065\n",
      "\n",
      "    accuracy                         0.8993     54924\n",
      "   macro avg     0.9388    0.7726    0.8220     54924\n",
      "weighted avg     0.9096    0.8993    0.8878     54924\n",
      "\n",
      "[[42798    61]\n",
      " [ 5469  6596]]\n",
      "Epoch 10/10\n",
      "859/859 [==============================] - 5s 6ms/step - loss: 0.2460 - acc: 0.9005 - val_loss: 0.2570 - val_acc: 0.8975\n",
      "215/215 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8868    0.9989    0.9395     42859\n",
      "         1.0     0.9929    0.5470    0.7054     12065\n",
      "\n",
      "    accuracy                         0.8996     54924\n",
      "   macro avg     0.9399    0.7730    0.8225     54924\n",
      "weighted avg     0.9101    0.8996    0.8881     54924\n",
      "\n",
      "[[42812    47]\n",
      " [ 5465  6600]]\n"
     ]
    }
   ],
   "source": [
    "#--------------------#\n",
    "# Import statements #\n",
    "#--------------------#\n",
    "\n",
    "import numpy as np\n",
    "from numpy.lib.recfunctions import repack_fields\n",
    "\n",
    "import scipy, argparse, itertools, os\n",
    "from scipy.io import arff\n",
    "\n",
    "import sklearn.utils\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Output\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Visualizations\n",
    "import matplotlib.pyplot as mp\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly as pl\n",
    "import plotly.offline as plo\n",
    "import plotly.graph_objs as plg\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "#Prediction Model\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense, Dropout,Activation, Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "import tensorflow.keras.callbacks\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "#--------------------#\n",
    "# Classes & Methods #\n",
    "#--------------------#\n",
    "\n",
    "#Class MaskedStandardScaler\n",
    "class MSS:\n",
    "\n",
    "  def fit_transform(self, Xs):\n",
    "    self.mean = Xs.mean(axis=0)\n",
    "    self.std = Xs.std(axis=0)\n",
    "    return self.transform(Xs)\n",
    "\n",
    "  def transform(self, Xs):\n",
    "    return (Xs - self.mean) / self.std\n",
    "\n",
    "#Class MaskedMinMaxScaler\n",
    "class MMMS:\n",
    "\n",
    "  def fit_transform(self, Xs):\n",
    "    self.min = Xs.min(axis=0)\n",
    "    self.max = Xs.max(axis=0)\n",
    "    return self.transform(Xs)\n",
    "\n",
    "  def transform(self, Xs):\n",
    "    return (Xs - self.min) / (self.max - self.min)\n",
    "\n",
    "def tocategoricalwithnans(Xs, n_cats):\n",
    "  categorized = np.zeros((len(Xs), n_cats))\n",
    "  for n, v in enumerate(np.array(Xs)):\n",
    "    if not np.isnan(v):\n",
    "      categorized[n, int(v)] = 1 \n",
    "  return categorized\n",
    "\n",
    "def pairwise(iterable):\n",
    "  a, b = itertools.tee(iterable)\n",
    "  next(b, None)\n",
    "  return zip(a, b)\n",
    "\n",
    "def splitdataset(data, labels, train_per_split, val_per_split, test_per_split):\n",
    "  assert (train_per_split + val_per_split + test_per_split) <= 1.0\n",
    "  \n",
    "  train_end_index = int(data.shape[0] * train_per_split)\n",
    "  valid_end_index = int(data.shape[0] * val_per_split) + train_end_index\n",
    "\n",
    "  X_train, Y_train = data[:train_end_index], labels[:train_end_index]\n",
    "  X_valid, Y_valid = data[train_end_index:valid_end_index], labels[train_end_index:valid_end_index]\n",
    "  X_test,  Y_test  = data[valid_end_index:], labels[valid_end_index:]\n",
    "  \n",
    "  return (X_train, X_valid, X_test), (Y_train, Y_valid, Y_test)\n",
    "\n",
    "def normalize(Xs, op, train_data_model):\n",
    "\n",
    "  ops = { \n",
    "    'mean'     : preprocessing.StandardScaler,\n",
    "    'minmax'   : preprocessing.MinMaxScaler,\n",
    "    'm-mean'   : MSS,\n",
    "    'm-minmax' : MMMS\n",
    "  }\n",
    "\n",
    "  if not train_data_model:\n",
    "    model = ops[op]()\n",
    "    Xs = model.fit_transform(Xs)\n",
    "    return Xs, model\n",
    "  else:\n",
    "    return train_data_model.transform(Xs), None\n",
    "\n",
    "def getclustermodelk(model, flags):\n",
    "  if flags.payload_kmeans_imputed:\n",
    "    return model.cluster_centers_.shape[0]\n",
    "  elif flags.payload_gmm_imputed:\n",
    "    return model.weights_.shape[0]\n",
    "\n",
    "def clusterpayloadfeatures(payloads, flags, train_data_model):\n",
    "  groups = grouppayloads(payloads)\n",
    "  groups_ids = [np.where(groups == i)[0] for i in range(3)]\n",
    "\n",
    "  only_nans = np.zeros_like(groups_ids[0])\n",
    "  only_pres = payloads[groups_ids[1]][:,-1].reshape((-1, 1))\n",
    "  wo_pres   = payloads[groups_ids[2]][:,:-1]\n",
    "\n",
    "  if not train_data_model:\n",
    "    \n",
    "    if flags.payload_kmeans_imputed:\n",
    "      print('Imputing payload with kmeans')\n",
    "      k1, k2 = flags.payload_kmeans_imputed\n",
    "      cluster_func = clusterkmeans\n",
    "    elif flags.payload_gmm_imputed:\n",
    "      print('Imputing payload with GMM')\n",
    "      k1, k2 = flags.payload_gmm_imputed\n",
    "      cluster_func = clustergmm\n",
    "\n",
    "    only_pres, only_pressure_cluster_model = cluster_func(only_pres, \\\n",
    "      'pressure', elbow=flags.elbow, k=k1)\n",
    "    wo_pres, wo_pressure_cluster_model = cluster_func(wo_pres, \\\n",
    "      'remaining payload features', elbow=flags.elbow, k=k2)\n",
    "\n",
    "  else:\n",
    "    only_pressure_cluster_model = train_data_model['only_pressure']\n",
    "    wo_pressure_cluster_model = train_data_model['wo_pressure']\n",
    "    only_pres = only_pressure_cluster_model.predict(only_pres)\n",
    "    wo_pres = wo_pressure_cluster_model.predict(wo_pres)\n",
    "\n",
    "  only_pres += 1\n",
    "  wo_pres += (getclustermodelk(only_pressure_cluster_model, flags) + 1)\n",
    "\n",
    "  combined_cat = np.concatenate((only_nans, only_pres, wo_pres))\n",
    "  combined_ind = np.concatenate(groups_ids)\n",
    "  combined = sorted(zip(combined_ind, combined_cat), key=lambda x: x[0])\n",
    "  combined = np.array([x[1] for x in combined])\n",
    "\n",
    "  models = {\n",
    "    'only_pressure' : only_pressure_cluster_model,\n",
    "    'wo_pressure' : wo_pressure_cluster_model\n",
    "  }\n",
    "\n",
    "  return np_utils.to_categorical(combined), models\n",
    "\n",
    "def preprocesspayloadfeaturesfulldataset(payloads, flags):\n",
    "\n",
    "\n",
    "  print('Imputing payload features - keeping old value')\n",
    "  \n",
    "  print('pressure measurement')\n",
    "  pressures_f = payloads[:,-1]\n",
    "  pressure_not_nans = np.where(~np.isnan(pressures_f))[0]\n",
    "  imputebykeepinglastvalue(pressures_f, pressure_not_nans)\n",
    "\n",
    "  print('binary payload')\n",
    "  binary_f = payloads[:,-4:-1]\n",
    "  binary_f_not_nans = np.where(~np.isnan(binary_f)[:,0])[0]\n",
    "  imputebykeepinglastvalue(binary_f, binary_f_not_nans)\n",
    "\n",
    "  print('system mode payload')\n",
    "  system_f = payloads[:,-5]\n",
    "  system_f_not_nans = np.where(~np.isnan(system_f))[0]\n",
    "  imputebykeepinglastvalue(system_f, system_f_not_nans)\n",
    "\n",
    "  print('real valued payload')\n",
    "  real_val_payloads_f = payloads[:,:-5]\n",
    "  real_val_payloads_f_not_nans = np.where(~np.isnan(real_val_payloads_f)[:,0])[0]\n",
    "  imputebykeepinglastvalue(real_val_payloads_f, real_val_payloads_f_not_nans)\n",
    "\n",
    "  '''restack'''\n",
    "  payloads = np.column_stack((\n",
    "    real_val_payloads_f,\n",
    "    system_f,\n",
    "    binary_f,\n",
    "    pressures_f\n",
    "  ))\n",
    "  return payloads\n",
    "\n",
    "\n",
    "def preprocessdatafulldataset(Xs, flags):\n",
    "  \n",
    "  addresses = Xs[:, 0].reshape((-1, 1))\n",
    "  responses = Xs[:, 2].reshape((-1, 1))\n",
    "\n",
    "  functions = Xs[:, 1].reshape((-1, 1))\n",
    "  payloads = Xs[:, 3:14]\n",
    "  \n",
    "  models = None\n",
    "\n",
    "  payloads  = preprocesspayloadfeaturesfulldataset(payloads, flags)\n",
    "\n",
    "  stacked = np.column_stack((\n",
    "    addresses, \n",
    "    functions, \n",
    "    responses,\n",
    "    payloads\n",
    "  ))\n",
    "\n",
    "  if Xs.shape[1] >= 14:\n",
    "    print('Additional features were found!')\n",
    "    remaining = Xs[:, 14:]\n",
    "    stacked = np.column_stack((stacked, remaining))\n",
    "\n",
    "  return stacked\n",
    "\n",
    "def preprocesspayloadfeatures(payloads, flags, train_data_model):\n",
    "\n",
    "  if flags.payload_indicator_imputed:\n",
    "    print('Imputing payload features using indicators')\n",
    "    indicators = np.isnan(payloads).astype(int)\n",
    "\n",
    "\n",
    "    '''split'''\n",
    "    realval_payload_f = payloads[:,:-5]\n",
    "    pressure_f = payloads[:,-1].reshape((-1, 1))\n",
    "\n",
    "    system_f = payloads[:,-5].reshape((-1, 1))\n",
    "    binary_payload_f = payloads[:,-4:-1]\n",
    "\n",
    "    '''categorize system_f feature'''\n",
    "    system_f = np.ma.array(system_f, mask=np.isnan(system_f)) \n",
    "    system_f = tocategoricalwithnans(system_f, 3)\n",
    "\n",
    "    '''masked normalization of realval payload features'''\n",
    "    op = 'm-' + flags.normalize\n",
    "    realval_payload_f = np.ma.array(realval_payload_f, mask=np.isnan(realval_payload_f)) \n",
    "    realval_payload_f, model = normalize(realval_payload_f, op, train_data_model)\n",
    "    realval_payload_f = np.array(realval_payload_f)\n",
    "\n",
    "    '''re-stack'''\n",
    "    payloads = np.column_stack((\n",
    "      realval_payload_f,\n",
    "      system_f,\n",
    "      binary_payload_f,\n",
    "      pressure_f\n",
    "    ))\n",
    "\n",
    "    '''replace remaining NaNs with 0'''\n",
    "    payloads[np.isnan(payloads)] = 0\n",
    "    return np.column_stack((payloads, indicators)), model\n",
    "\n",
    "  elif flags.payload_keep_value_imputed:\n",
    "    print('Imputing payload features - keeping old value')\n",
    "    \n",
    "    print('pressure measurement')\n",
    "    pressures_f = payloads[:,-1]\n",
    "\n",
    "    print('binary payload')\n",
    "    binary_f = payloads[:,-4:-1]\n",
    "\n",
    "    print('system mode payload')\n",
    "    system_f = payloads[:,-5]\n",
    "\n",
    "    print('real valued payload')\n",
    "    real_val_payloads_f = payloads[:,:-5]\n",
    "\n",
    "    op = flags.normalize\n",
    "    real_val_payloads_f, model = normalize(real_val_payloads_f, op, train_data_model)\n",
    "\n",
    "    system_f = tocategoricalwithnans(system_f, 3)\n",
    "\n",
    "    '''restack'''\n",
    "    payloads = np.column_stack((\n",
    "      real_val_payloads_f,\n",
    "      system_f,\n",
    "      binary_f,\n",
    "      pressures_f\n",
    "    ))\n",
    "    return payloads, model\n",
    "\n",
    "  else:\n",
    "    return clusterpayloadfeatures(payloads, flags, train_data_model)\n",
    "\n",
    "def imputebykeepinglastvalue(features, not_nans):\n",
    "  first_not_nan = not_nans[0]\n",
    "  features[:first_not_nan] = features[first_not_nan]\n",
    "\n",
    "  for begin, end in pairwise(not_nans):\n",
    "    features[begin:end] = features[begin]\n",
    "\n",
    "  '''\n",
    "  Case: To keep the value until the end of dataset\n",
    "  '''\n",
    "  last = len(features)\n",
    "  last_not_nan = not_nans[-1]\n",
    "  if last != last_not_nan:\n",
    "    features[last_not_nan+1:] = features[last_not_nan]\n",
    "\n",
    "def grouppayloads(payloads):\n",
    "  '''\n",
    "   Last two columns uniquely identify a category of payload\n",
    "    both columns are NaNs -> 0\n",
    "    first is NaN and second is not NaN -> 1\n",
    "    first is not NaN and second is NaN -> 2\n",
    "  Returns indicies\n",
    "  '''\n",
    "  ids = payloads[:,-2:]\n",
    "  ids = np.packbits(~np.isnan(ids), axis=1) // 64\n",
    "  return ids.reshape(-1)\n",
    "\n",
    "def preprocessfunctioncodes(functions, flags, train_data_model):\n",
    "\n",
    "  if flags.encode_function:    \n",
    "    '''\n",
    "    Encode function codes is the same for training and non-training data\n",
    "    '''\n",
    "    print('Encoding function...')\n",
    "    encoder = LabelEncoder()\n",
    "    functions = encoder.fit_transform(functions.reshape(-1))\n",
    "    return np_utils.to_categorical(functions), None\n",
    "\n",
    "  if not train_data_model:\n",
    "\n",
    "    if flags.cluster_function_kmeans != None:\n",
    "      k = flags.cluster_function_kmeans\n",
    "      predicted, model = clusterkmeans(functions, 'function', elbow=flags.elbow, k=k)\n",
    "      return np_utils.to_categorical(predicted), model\n",
    "\n",
    "    elif flags.cluster_function_gmm:\n",
    "      k = flags.cluster_function_gmm\n",
    "      predicted, model = clustergmm(functions, 'function', elbow=flags.elbow, k=k)\n",
    "      return np_utils.to_categorical(predicted), model\n",
    "\n",
    "  else:\n",
    "    return np_utils.to_categorical(train_data_model.predict(functions)), None\n",
    "\n",
    "def preprocessdata(Xs, flags, train_data_models = None):\n",
    "  \n",
    "  addresses = Xs[:, 0].reshape((-1, 1))\n",
    "  responses = Xs[:, 2].reshape((-1, 1))\n",
    "\n",
    "  functions = Xs[:, 1].reshape((-1, 1))\n",
    "  payloads = Xs[:, 3:14]\n",
    "  \n",
    "  models = None\n",
    "\n",
    "  if not train_data_models:\n",
    "    functions, f_model = preprocessfunctioncodes(functions, flags, None)\n",
    "    payloads, p_model  = preprocesspayloadfeatures(payloads, flags, None)\n",
    "    models = {'function': f_model, 'payload': p_model}\n",
    "\n",
    "  else:\n",
    "    f_model = train_data_models['function']\n",
    "    p_model = train_data_models['payload']\n",
    "\n",
    "    functions, _ = preprocessfunctioncodes(functions, flags, f_model)\n",
    "    payloads, _  = preprocesspayloadfeatures(payloads, flags, p_model)  \n",
    "\n",
    "  stacked = np.column_stack((\n",
    "    addresses, \n",
    "    functions, \n",
    "    responses,\n",
    "    payloads\n",
    "  ))\n",
    "\n",
    "  if Xs.shape[1] >= 14:\n",
    "    print('Additional features were found!')\n",
    "    remaining = Xs[:, 14:]\n",
    "\n",
    "    if not train_data_models:\n",
    "      remaining, r_model = normalize(remaining, flags.normalize, None)\n",
    "\n",
    "      models['remaining'] = r_model\n",
    "    else:\n",
    "      r_model = train_data_models['remaining']\n",
    "      remaining, _ = normalize(remaining, flags.normalize, r_model)\n",
    "\n",
    "    stacked = np.column_stack((stacked, remaining))\n",
    "\n",
    "  return stacked, models\n",
    "\n",
    "def cluster(model_class, score_func, Xs, feature_name, k, elbow, max_k):\n",
    "  \n",
    "  if elbow:\n",
    "    models = [model_class(i).fit(Xs) for i in range(1, max_k+1)]\n",
    "    scores = [score_func(model, Xs) for model in models]\n",
    "    k = promptelbowmethod(scores, feature_name)\n",
    "\n",
    "  m = model_class(k).fit(Xs)\n",
    "  return m.predict(Xs), m\n",
    "\n",
    "def clustergmm(Xs, feature_name, k=None, elbow=True):\n",
    "  \n",
    "  print('Clustering {} with GMM'.format(feature_name))\n",
    "  GMM = lambda k: GaussianMixture(n_components=k, init_params='random')\n",
    "  score_func = lambda gmm, Xs: gmm.aic(Xs)\n",
    "  return cluster(GMM, score_func, Xs, feature_name, k, elbow, max_k=20)\n",
    "\n",
    "def clusterkmeans(Xs, feature_name, k=None, elbow=True):\n",
    "  \n",
    "  print('Clustering {} with Kmeans'.format(feature_name))\n",
    "  KM = lambda k: KMeans(n_clusters=k)\n",
    "  score_func = lambda km, Xs: km.score(Xs)\n",
    "  return cluster(KM, score_func, Xs, feature_name, k, elbow, max_k=10)\n",
    "\n",
    "def promptelbowmethod(scores, feature_name):\n",
    "  print(scores)\n",
    "  xs = range(1, len(scores)+1)\n",
    "  ys = np.array(scores)\n",
    "  is_log = ''\n",
    "  if np.all(ys > 0) or np.all(ys < 0):\n",
    "    ys = np.log(np.abs(ys))\n",
    "    is_log = 'log'\n",
    "  print(ys)\n",
    "  plt.plot(xs, ys)\n",
    "  plt.xticks(xs)\n",
    "  plt.xlabel('Number of clusters')\n",
    "  plt.ylabel('{} Score'.format(is_log))\n",
    "  plt.title('Elbow method for {}'.format(feature_name))\n",
    "  plt.show()\n",
    "  print('Please enter number of clusters for {}:'.format(feature_name))\n",
    "  k = int(input('-->'))\n",
    "  print('\\nselected k: {}'.format(k))\n",
    "  return k\n",
    "\n",
    "#-----------------------------------#\n",
    "# Run Data Pre-processing Main Code #\n",
    "#-----------------------------------#\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser(description='Preprocess SCADA dataset.')\n",
    "  parser.add_argument('-d', '--dataset', type=str, help='Dataset filename')\n",
    "  parser.add_argument('--label', type=str, choices=['binary', 'category', 'dcategory'],\n",
    "    default='binary', help='Type of labels to output in the preprocessed dataset')\n",
    "  parser.add_argument('-t', '--time-series', action='store_true', \n",
    "    help='Keep temporal structure of the data')\n",
    "  parser.add_argument('-n', '--normalize', type=str, choices=['mean', 'minmax'],\n",
    "    default='mean', help='Type of normalization to preform')\n",
    "  parser.add_argument('--discard-crc', action='store_true', \n",
    "    help='Remove CRC feature from the dataset')\n",
    "  parser.add_argument('--discard-length', action='store_true', \n",
    "    help='Remove Length feature from the dataset')\n",
    "\n",
    "  function_encoding_group = parser.add_mutually_exclusive_group(required=True)\n",
    "  function_encoding_group.add_argument('--encode-function', action='store_true',\n",
    "    help='Encode function codes with One-Hot encoding')\n",
    "  function_encoding_group.add_argument('--cluster-function-kmeans', type=int,\n",
    "    help='Cluster function codes with kmeans')\n",
    "  function_encoding_group.add_argument('--cluster-function-gmm', type=int,\n",
    "    help='Cluster function codes with GMM')\n",
    "\n",
    "  payload_features_impution_group = parser.add_mutually_exclusive_group(required=True)\n",
    "  payload_features_impution_group.add_argument('--payload-keep-value-imputed', action='store_true',\n",
    "    help='Impute payload features by keeping the oldest value')\n",
    "  payload_features_impution_group.add_argument('--payload-indicator-imputed', action='store_true',\n",
    "    help='Impute payload features with 0\\'s and add indicators')    \n",
    "  payload_features_impution_group.add_argument('--payload-kmeans-imputed', type=int, nargs=2,\n",
    "    help='Impute payload features by clustering with kmeans, first for pressure second for remaining')\n",
    "  payload_features_impution_group.add_argument('--payload-gmm-imputed', type=int, nargs=2,\n",
    "    help='Impute payload features by clustering with GMM, first for pressure second for remaining')    \n",
    "\n",
    "  parser.add_argument('--elbow', action='store_true', \n",
    "    help='When using clustering, use elbow method to get best number of clusters')\n",
    "\n",
    "  parser.add_argument('-o', '--output', type=str, help='Output .npy files')\n",
    "  parser.add_argument('--split', type=float, nargs=3, help='train/val/test split ratio')\n",
    "\n",
    "  flags = parser.parse_args('-d ./dataset/IanArffDataset.arff --split 0.6 0.2 0.2 --encode-function --label binary --time-series -n mean --payload-indicator-imputed  -o processeddataset'.split())\n",
    "\n",
    "  dataset, meta = arff.loadarff(flags.dataset)\n",
    "\n",
    "  label_types = {\n",
    "    'binary'    : 'binary result',\n",
    "    'category'  : 'categorized result',\n",
    "    'dcategory' : 'specific result'\n",
    "  }\n",
    "\n",
    "  print('____________________________________________')\n",
    "  print('START Data Pre-processing')\n",
    "  print('____________________________________________')\n",
    "  print('Encoding labels...')\n",
    "  label_name = label_types[flags.label]\n",
    "  labels = dataset[label_name].astype(np.float)\n",
    "  labels = labels.reshape((-1, 1))\n",
    "  \n",
    "  \n",
    "  print('Encoding addressess...')\n",
    "  #pre-process address before splitting, simple change of values and reshape into column vec\n",
    "  addresses = preprocessing.label_binarize(dataset['address'], classes=[4])\n",
    "\n",
    "  print('Encoding functions...')\n",
    "  functions = dataset['function'].astype(np.float).reshape((-1, 1))\n",
    "\n",
    "  print('Encoding command responses...')\n",
    "  #pre-process address before splitting, parse string and reshape into column vec\n",
    "  responses = dataset['command response'].astype(np.float).reshape((-1, 1))\n",
    "\n",
    "  print('Extracting payload features')\n",
    "  payload_feature_names = meta.names()[3:14]\n",
    "    \n",
    "  payload_features = repack_fields(dataset[payload_feature_names])\n",
    "  \n",
    "  payload_features = payload_features \\\n",
    "    .view(np.float64) \\\n",
    "    .reshape(payload_features.shape + (-1,))\n",
    "\n",
    "  Xs = np.column_stack((\n",
    "    addresses, \n",
    "    functions, \n",
    "    responses,\n",
    "    payload_features\n",
    "  ))\n",
    "\n",
    "  if not flags.discard_length:\n",
    "    print('\\tAdding length...')\n",
    "    lengths = dataset['length'].astype(np.float).reshape((-1, 1))\n",
    "    Xs = np.column_stack((Xs, lengths))\n",
    "\n",
    "  if not flags.discard_crc:\n",
    "    print('\\tAdding crc rate...')\n",
    "    crcs = dataset['crc rate'].astype(np.float).reshape((-1, 1))\n",
    "    Xs = np.column_stack((Xs, crcs))\n",
    "\n",
    "  if flags.time_series:\n",
    "    print('\\tAdding timestamp differences...')\n",
    "    Xs = np.column_stack((Xs, dataset['time']))\n",
    "    \n",
    "\n",
    "  if flags.payload_keep_value_imputed:\n",
    "\n",
    "    Xs = preprocessdatafulldataset(Xs,flags)\n",
    "    Xs, labels = sklearn.utils.shuffle(Xs, labels)\n",
    "\n",
    "    print('\\n\\tSplitting dataset...')\n",
    "    Xs, Ys = splitdataset(Xs, labels, *flags.split)\n",
    "    Xs_train, Xs_val, Xs_test = Xs\n",
    "    Ys_train, Ys_val, Ys_test = Ys\n",
    "\n",
    "    Xs_train_val = np.concatenate((Xs_train,Xs_val))\n",
    "    Ys_train_val = np.concatenate((Ys_train,Ys_val))\n",
    "\n",
    "    Xs_train_test = Xs_test\n",
    "    Ys_train_test = Ys_test\n",
    "\n",
    "    Xs_train, train_models = preprocessdata(Xs_train, flags)\n",
    "    Xs_train_val, train_models_val = preprocessdata(Xs_train_val, flags)\n",
    "    Xs_val,  _ = preprocessdata(Xs_val,  flags, train_data_models=train_models)\n",
    "    Xs_test, _ = preprocessdata(Xs_test, flags, train_data_models=train_models)\n",
    "    Xs_train_test, _ = preprocessdata(Xs_train_test, flags, train_data_models=train_models_val)\n",
    "\n",
    "  else:\n",
    "    print('\\n\\tSplitting dataset...')\n",
    "    Xs, labels = sklearn.utils.shuffle(Xs, labels)\n",
    "    Xs, Ys = splitdataset(Xs, labels, *flags.split)\n",
    "    Xs_train, Xs_val, Xs_test = Xs\n",
    "    Ys_train, Ys_val, Ys_test = Ys\n",
    "\n",
    "    Xs_train_val = np.concatenate((Xs_train,Xs_val))\n",
    "    Ys_train_val = np.concatenate((Ys_train,Ys_val))\n",
    "    Xs_train_test = Xs_test\n",
    "    Ys_train_test = Ys_test\n",
    "\n",
    "    Xs_train, train_models = preprocessdata(Xs_train, flags)\n",
    "    Xs_val,  _ = preprocessdata(Xs_val,  flags, train_data_models=train_models)\n",
    "    Xs_test, _ = preprocessdata(Xs_test, flags, train_data_models=train_models)\n",
    "\n",
    "    Xs_train_val, train_models_val = preprocessdata(Xs_train_val, flags)\n",
    "    Xs_train_test, _ = preprocessdata(Xs_train_test, flags, train_data_models=train_models_val)\n",
    "\n",
    "    print(Xs_train_val.shape, Ys_train_val.shape)\n",
    "    print(Xs_train_test.shape, Ys_train_test.shape)\n",
    "\n",
    "\n",
    "  if not flags.time_series:\n",
    "    Xs_train, Ys_train = sklearn.utils.shuffle(Xs_train, Ys_train, random_state=0)\n",
    " \n",
    "  output_dir = flags.output\n",
    "\n",
    "  if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "  np.save(os.path.join(output_dir, 'Xs_train'), Xs_train)\n",
    "  np.save(os.path.join(output_dir, 'Xs_val'),   Xs_val)\n",
    "  np.save(os.path.join(output_dir, 'Xs_test'),  Xs_test)\n",
    "\n",
    "  np.save(os.path.join(output_dir, 'Ys_train'), Ys_train)\n",
    "  np.save(os.path.join(output_dir, 'Ys_val'),   Ys_val)\n",
    "  np.save(os.path.join(output_dir, 'Ys_test'),  Ys_test)\n",
    "\n",
    "  np.save(os.path.join(output_dir, 'Xs_train_val'), Xs_train_val)\n",
    "  np.save(os.path.join(output_dir, 'Xs_train_test'), Xs_train_test)\n",
    "  np.save(os.path.join(output_dir, 'Ys_train_val'), Ys_train_val)\n",
    "  np.save(os.path.join(output_dir, 'Ys_train_test'), Ys_train_test)\n",
    "    \n",
    "  print('____________________________________________')\n",
    "  print('END Data Pre-processing')\n",
    "  print('____________________________________________')\n",
    "    \n",
    "    \n",
    "#-----------------------#  \n",
    "# PREDICTION MODEL CODE #\n",
    "#-----------------------#\n",
    "\n",
    "class ReportMetric(tensorflow.keras.callbacks.Callback):\n",
    "\n",
    "  def __init__(self, valid_data, label_dim, bs, file):\n",
    "    self.valid_data = valid_data\n",
    "    self.label_dim = label_dim\n",
    "    self.bs = bs\n",
    "    self.file = file\n",
    "\n",
    "  def on_epoch_end(self, batch, logs={}):\n",
    "    if self.label_dim != 1:\n",
    "      Y_pred = np.argmax(model.predict(self.valid_data[0], batch_size=self.bs, verbose=1), axis=-1).reshape(-1)\n",
    "      Y_true = np.argmax(self.valid_data[1], axis=2).reshape(-1)\n",
    "    else:\n",
    "      Y_pred = (model.predict(self.valid_data[0], batch_size=self.bs, verbose=1) > 0.5).astype(\"int32\").reshape(-1)\n",
    "      Y_true = self.valid_data[1].reshape(-1)\n",
    "\n",
    "    report = classification_report(Y_true, Y_pred,digits=4)\n",
    "    conf_matrix  = confusion_matrix(Y_true, Y_pred)\n",
    "    self.file.write('\\n' + report + '\\n')\n",
    "    print(report)\n",
    "    print(conf_matrix)\n",
    "\n",
    "def make_sequences(Xs, Ys, seqlen, step = 1):\n",
    "  Xseq, Yseq = [], []\n",
    "  for i in range(0, Xs.shape[0] - seqlen + 1, step):\n",
    "    Xseq.append(Xs[i: i+seqlen])\n",
    "    Yseq.append(Ys[i: i+seqlen])\n",
    "  return np.array(Xseq), np.array(Yseq)\n",
    "\n",
    "def lstm_model(input_dim, output_dim, seq_len, hidden=128, dropout=0.0, lr=0.1):\n",
    "  model = Sequential()\n",
    "  layers = {'input': input_dim, 'hidden': hidden, 'output': output_dim}\n",
    "  \n",
    "  model.add(Bidirectional(LSTM(layers['input'], return_sequences=True),\n",
    "    merge_mode='concat', input_shape=(seq_len, layers['input'])))\n",
    "  model.add(Dropout(dropout))\n",
    "\n",
    "  activation = 'softmax' if output_dim > 1 else 'sigmoid'\n",
    "  loss = 'categorical_crossentropy' if output_dim > 1 else 'binary_crossentropy'\n",
    "\n",
    "  model.add(TimeDistributed(Dense(layers['output'], activation=activation)))\n",
    "\n",
    "  model.compile(loss=loss, optimizer=Adam(lr=lr), metrics=['acc'])\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser(description='Run LSTM')\n",
    "  parser.add_argument('-d', '--dataset', required=True, type=str, help='Data directory')\n",
    "  parser.add_argument('-i', '--iters', default=20, type=int, help='Number of random samples')\n",
    "  \n",
    "  flags = parser.parse_args('-d ./processeddataset/ --i 2'.split())\n",
    "\n",
    "  dataset_dir = flags.dataset\n",
    "  dataset_filenames = [\n",
    "    'Xs_train.npy', 'Xs_val.npy', 'Xs_test.npy', 'Xs_train_val.npy','Xs_train_test.npy',\n",
    "    'Ys_train.npy', 'Ys_val.npy', 'Ys_test.npy', 'Ys_train_val.npy', 'Ys_train_test.npy'\n",
    "  ]\n",
    "  dataset_filenames = map(lambda x: os.path.join(dataset_dir, x), dataset_filenames)\n",
    "  X_train, X_valid, X_test, Xs_train_val, Xs_train_test, Y_train, Y_valid, Y_test, Ys_train_val, Ys_train_test  = map(np.load, dataset_filenames)\n",
    "  \n",
    "  # For evaluation on test set - uncomment below code #\n",
    "  X_train = Xs_train_val\n",
    "  Y_train = Ys_train_val\n",
    "  X_test = Xs_train_test\n",
    "  Y_test = Ys_train_test\n",
    "\n",
    "  print('____________________________________________')\n",
    "  print('START Building Model')\n",
    "  print('____________________________________________')\n",
    "    \n",
    "  print(X_train.shape, Y_train.shape)\n",
    "  print(X_valid.shape, Y_valid.shape)\n",
    "  print(X_test.shape, Y_test.shape)\n",
    "\n",
    "  n_label, label_dim = Y_train.shape\n",
    "  sums = np.sum(Y_train, axis=0)\n",
    "  \n",
    "  print('Label dimension: {}'.format(label_dim))\n",
    "  if not label_dim in [1, 8, 36]:\n",
    "    raise Exception('!Unknown label dimension: {}'.format(label_dim))\n",
    "\n",
    "  input_dim = X_train.shape[1]\n",
    "  print('Input dimension: {}'.format(input_dim))\n",
    "\n",
    "  iters = flags.iters\n",
    "  print('Number of iterations: {}'.format(iters))\n",
    "  learning_rate = lambda: 10 ** np.random.uniform(-3, -1)\n",
    "  sequence_length = lambda: np.random.choice([4])\n",
    "  hidden_layer_size = lambda: int(2 ** np.random.uniform(6, 8))\n",
    "  batch_size = lambda: int(2 ** np.random.uniform(6, 8))\n",
    "  dropout = lambda: np.random.uniform(0.0, 0.2)\n",
    "  step = lambda: 4\n",
    "  n_epochs = lambda: 10\n",
    "  \n",
    "  ranges = [\n",
    "    learning_rate,\n",
    "    batch_size,\n",
    "    n_epochs,\n",
    "    sequence_length,\n",
    "    dropout,\n",
    "    hidden_layer_size,\n",
    "    step\n",
    "  ]\n",
    "\n",
    "  with open(os.path.join(dataset_dir, 'aggregate.txt'), 'w') as f:\n",
    "    f.write('Learning Rate,Batch size,Number of epochs,Sequence length,Dropout rate,Hidden layer size\\n')\n",
    "\n",
    "  for iterations in range(iters):\n",
    "    #p() is calling all functions in the list\n",
    "    hyperparams = [p() for p in ranges]\n",
    "    (lr, bs, ne, sl, dr, hls, step) = hyperparams\n",
    "    hparams = ','.join(map(str, hyperparams))\n",
    "\n",
    "    print('Hyperparameters setting:')\n",
    "    print(hparams)\n",
    "\n",
    "    output_dir = os.path.join(dataset_dir, 'results')\n",
    "    if not os.path.exists(output_dir):\n",
    "      os.makedirs(output_dir)\n",
    "\n",
    "    model = lstm_model(input_dim, label_dim, sl, \\\n",
    "      hidden=hls, dropout=dr, lr=lr)\n",
    "\n",
    "    print('Preparing sequences - Length = {}'.format(sl))\n",
    "\n",
    "    X_train_seq, Y_train_seq = make_sequences(X_train, Y_train, sl, step)\n",
    "    X_valid_seq, Y_valid_seq = make_sequences(X_valid, Y_valid, sl, step)\n",
    "    X_test_seq,  Y_test_seq  = make_sequences(X_test,  Y_test,  sl, step)\n",
    "    \n",
    "    print('____________________________________________')\n",
    "    print('MODEL EVALUATION')\n",
    "    print('____________________________________________')\n",
    "\n",
    "    print(X_train_seq.shape,Y_train_seq.shape)\n",
    "    print(X_valid_seq.shape,Y_valid_seq.shape)\n",
    "    print(X_test_seq.shape, Y_test_seq.shape)\n",
    "\n",
    "    output_file = os.path.join(output_dir, '-'.join(map(str, hyperparams)) + '.txt')\n",
    "    with open(output_file, 'w') as f:\n",
    "\n",
    "      log_dir = os.path.join(output_dir, '_'.join(map(str, hyperparams)))\n",
    "      if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "      \n",
    "      chpt_filepath = os.path.join(log_dir, 'weights.{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "\n",
    "      smcb = tensorflow.keras.callbacks.ModelCheckpoint(chpt_filepath)\n",
    "      tbcb = tensorflow.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True)\n",
    "      valid_data = (X_valid_seq, Y_valid_seq)\n",
    "      rmcb = ReportMetric(valid_data, label_dim, bs, f) \n",
    "\n",
    "      reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=3, min_lr=0.001)\n",
    "\n",
    "      # Training Set:\n",
    "    \n",
    "#       h = model.fit(X_train_seq, Y_train_seq, \\\n",
    "#         validation_data=(X_valid_seq, Y_valid_seq), batch_size=bs, epochs=ne, \\\n",
    "#         callbacks=[tbcb, smcb, rmcb, reduce_lr]\n",
    "#       )\n",
    "\n",
    "      # For Test set - uncomment below code\n",
    "\n",
    "      h = model.fit(X_train_seq, Y_train_seq, \\\n",
    "        validation_data=(X_test_seq, Y_test_seq), batch_size=bs, epochs=ne, \\\n",
    "        callbacks=[tbcb, smcb, rmcb, reduce_lr]\n",
    "      )\n",
    "    \n",
    "      for k,v in h.history.items():\n",
    "        f.write(k + ':' + ','.join(map(str, v)) + '\\n')\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
